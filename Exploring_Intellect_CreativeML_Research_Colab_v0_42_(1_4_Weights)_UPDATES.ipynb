{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/digitalartslab/EIE_CML_Research/blob/main/Exploring_Intellect_CreativeML_Research_Colab_v0_42_(1_4_Weights)_UPDATES.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SKQ4bH7qMGrA"
      },
      "source": [
        "# Exploring Intellect CreativeML Research Colab v0.42 (1.4 Weights) - Stable Diffusion Model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h-qNQtzBRbjo"
      },
      "source": [
        "#DARL"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PmIPmy4jRegy"
      },
      "source": [
        "The results of the output of our research will be found here: https://www.darl.africa . This will include any collaborations we do with any other artist using this research\n",
        "\n",
        "If you want to share your prompts or research with us email: info@darl.africa "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o5V9MWbFtpNi"
      },
      "source": [
        "#GPU Info\n",
        " (If it throws an error here, go to Runtime, then click \"Change Runtime Type\" and then select \"GPU\"). There's also a chance that Colab put you on a GPU timeout if this is set and it still throws an error"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qxlZ660jRzVg"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ekR-LW6trWG"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHaZZ0uKti1c"
      },
      "source": [
        "# Diffusers Method\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NUlgKvW_EfqX"
      },
      "source": [
        "Merged everything into one cell. Just click Render and you are good-to-go. First run will take a while since it has to setup everything, but afterwards it should be quick to render with every go-around (or at least until the session disconnects)\n",
        "\n",
        "If anyone having problems running this colab on mobile device then try checking the `Desktop Site` in chrome from the menu from top right corner . (Kudos to Rohan Singh)\n",
        "\n",
        "Went back to speedy mode, but will take longer for the program to actually stop when you hit stop (Can't find a good solution for this). When you spin it up again though it should be lightning fast. I THINK I have the stuck vram issue mitigated, but not sure\n",
        "\n",
        "Will also be looking at adding a mobile friendly drawing feature\n",
        "\n",
        "<b>If you have colab connectivity issues, keep the `NUM_ITERS` low!</b>\n",
        "\n",
        "\n",
        "Currently figuring out how to best include textual inversion into the notebook, in the meantime if you want to try it, here are the notebooks:\n",
        "\n",
        "Training: https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/sd_textual_inversion_training.ipynb\n",
        "\n",
        "Inference: https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/stable_conceptualizer_inference.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ucr5_i21xSjv"
      },
      "outputs": [],
      "source": [
        "#@title Render Images\n",
        "class Cleaner:\n",
        "  def clean_env():\n",
        "    gc, torch = Manager.manage_imports('clean_env')\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "class Colab:\n",
        "  def __init__(self):\n",
        "    self.settings = self.UserSettings.set_settings()\n",
        "\n",
        "  def clear():\n",
        "    from IPython.display import clear_output; clear_output()\n",
        "\n",
        "  def manage_drive(drive_pic_dir):\n",
        "    exists, mount, makedirs = Manager.manage_imports('manage_drive')\n",
        "    if not exists('/content/drive'):\n",
        "      mount('/content/drive')\n",
        "    if not exists(f'/content/drive/MyDrive/{drive_pic_dir}'):\n",
        "      makedirs(f'/content/drive/MyDrive/{drive_pic_dir}')\n",
        "\n",
        "  class Images:\n",
        "    def resize_image():\n",
        "      pass\n",
        "\n",
        "    def suggest_resolution():\n",
        "      pass\n",
        "      \n",
        "    class Painter:\n",
        "      def inpaint(width, height):\n",
        "        import requests\n",
        "        from io import BytesIO\n",
        "        def draw(filename='drawing.png', color=\"white\", w=256, h=256, line_width=50,loop=False, init_img=\"init.jpg\"):\n",
        "          filename=\"init.jpg\"\n",
        "          import google\n",
        "          from IPython.display import HTML\n",
        "          from base64 import b64decode\n",
        "          import os\n",
        "          import shutil\n",
        "          import uuid\n",
        "          COLAB_HTML_ROOT = \"/usr/local/share/jupyter/nbextensions/google.colab/\"\n",
        "\n",
        "          def moveToExt(filename:str) -> str:\n",
        "            if not os.path.exists(filename):\n",
        "              print(\"Image file not found\")\n",
        "              return None\n",
        "            \n",
        "            target = os.path.basename(filename)\n",
        "            target = os.path.join(COLAB_HTML_ROOT, str(uuid.uuid4()) + target)\n",
        "            \n",
        "            shutil.copyfile(filename,target)\n",
        "            print(\"moved to ext\")\n",
        "            return target\n",
        "          real_filename = os.path.realpath(filename)\n",
        "          html_filename = real_filename\n",
        "          html_real_filename = html_filename\n",
        "          if os.path.exists(real_filename):\n",
        "            html_real_filename = moveToExt(real_filename)\n",
        "            html_filename = html_real_filename.replace(\"/usr/local/share/jupyter\",\"\")\n",
        "            \n",
        "\n",
        "          canvas_html = f\"\"\"\n",
        "        <canvas width={w} height={h}></canvas>\n",
        "\n",
        "        <div class=\"slidecontainer\">\n",
        "        <label for=\"lineWidth\" id=\"lineWidthLabel\">{line_width}px</label>\n",
        "          <input type=\"range\" min=\"1\" max=\"100\" value=\"1\" class=\"slider\" id=\"lineWidth\">\n",
        "        </div>\n",
        "\n",
        "        <div>\n",
        "          <button id=\"loadImage\">Reload from disk</button>\n",
        "          <button id=\"reset\">Reset</button>\n",
        "          <button id=\"save\">Save</button>\n",
        "        </div>\n",
        "        <script>\n",
        "\n",
        "        function loadImage(url) {{\n",
        "        return new Promise(r => {{ let i = new Image(); i.onload = (() => r(i)); i.src = url; }});\n",
        "        }}\n",
        "          \n",
        "          \n",
        "          var canvas = document.querySelector('canvas')\n",
        "          var ctx = canvas.getContext('2d')\n",
        "          ctx.lineWidth = {line_width};\n",
        "          \n",
        "          ctx.fillRect(0, 0, canvas.width, canvas.height);\n",
        "          ctx.strokeStyle = \"{color}\";\n",
        "\n",
        "\n",
        "          var slider = document.getElementById(\"lineWidth\");\n",
        "          slider.oninput = function() {{\n",
        "            ctx.lineWidth = this.value;\n",
        "            lineWidthLabel.innerHTML = `${{this.value}}px`\n",
        "          }}\n",
        "\n",
        "\n",
        "          function updateStroke(event){{\n",
        "              ctx.strokeStyle = event.target.value\n",
        "          }}\n",
        "          function updateBG(event){{\n",
        "              ctx.fillStyle = event.target.value\n",
        "          }}\n",
        "          \n",
        "          \n",
        "          var clear_button = document.querySelector('#reset')\n",
        "          var reload_img_button = document.querySelector('#loadImage')\n",
        "          \n",
        "          var button = document.querySelector('#save')\n",
        "\n",
        "          var mouse = {{x: 0, y: 0}}\n",
        "          canvas.addEventListener('mousemove', function(e) {{\n",
        "            mouse.x = e.pageX - this.offsetLeft\n",
        "            mouse.y = e.pageY - this.offsetTop\n",
        "          }})\n",
        "          canvas.onmousedown = ()=>{{\n",
        "            ctx.beginPath()\n",
        "            ctx.moveTo(mouse.x, mouse.y)\n",
        "            canvas.addEventListener('mousemove', onPaint)\n",
        "          }}\n",
        "          canvas.onmouseup = ()=>{{\n",
        "            canvas.removeEventListener('mousemove', onPaint)\n",
        "          }}\n",
        "          var onPaint = ()=>{{\n",
        "            ctx.lineTo(mouse.x, mouse.y)\n",
        "            ctx.stroke()\n",
        "          }}\n",
        "          reload_img_button.onclick = async ()=>{{\n",
        "            console.log(\"Reloading Image {html_filename}\")\n",
        "            let img = await loadImage('{html_filename}'); \n",
        "            console.log(\"Loaded image\")\n",
        "            ctx.drawImage(img, 0, 0);\n",
        "\n",
        "          }}\n",
        "          reload_img_button.click()\n",
        "        \n",
        "          clear_button.onclick = ()=>{{\n",
        "              console.log('Clearing Screen')\n",
        "              ctx.clearRect(0, 0, canvas.width, canvas.height);\n",
        "              ctx.fillRect(0, 0, canvas.width, canvas.height);\n",
        "            }}\n",
        "            canvas.addEventListener('load', function() {{\n",
        "            console.log('All assets are loaded')\n",
        "          }})\n",
        "          var data = new Promise(resolve=>{{\n",
        "            button.onclick = ()=>{{\n",
        "\n",
        "              var c = ctx\n",
        "              var imageData = ctx.getImageData(0,0, {w}, {h});\n",
        "              var pixel = imageData.data;\n",
        "              var r=0, g=1, b=2,a=3;\n",
        "            for (var p = 0; p<pixel.length; p+=4)\n",
        "            {{\n",
        "              if (\n",
        "                  pixel[p+r] != 255 &&\n",
        "                  pixel[p+g] != 255 &&\n",
        "                  pixel[p+b] != 255) \n",
        "              {{pixel[p+r] =0; pixel[p+g]=0; pixel[p+b]=0}}\n",
        "            }}\n",
        "\n",
        "            c.putImageData(imageData,0,0);\n",
        "              resolve(canvas.toDataURL('image/png'))\n",
        "            }}\n",
        "            \n",
        "          }})\n",
        "          \n",
        "          \n",
        "        </script>\n",
        "        \"\"\"\n",
        "          print(HTML)\n",
        "          display(HTML(canvas_html))\n",
        "          print(\"Evaluating JS\")\n",
        "          \n",
        "          data = google.colab.output.eval_js(\"data\")\n",
        "          if data:\n",
        "            print(\"Saving Sketch\")  \n",
        "            binary = b64decode(data.split(',')[1])\n",
        "            # filename = html_real_filename if loop else filename\n",
        "            with open(\"init_mask.png\", 'wb') as f:\n",
        "              f.write(binary)\n",
        "            #return len(binary)\n",
        "\n",
        "\n",
        "\n",
        "        draw(filename = \"init_mask.png\", w=width, h=height)\n",
        "        import PIL.Image\n",
        "        return PIL.Image.open('init_mask.png')\n",
        "      def img2img(width, height):\n",
        "        import os\n",
        "        os.chdir('/content/')\n",
        "        def draw(filename='drawing.png', color=\"black\", bg_color=\"transparent\",w=256, h=256, line_width=1,loop=False):\n",
        "          import google\n",
        "          from IPython.display import HTML\n",
        "          from base64 import b64decode\n",
        "          import os\n",
        "          import shutil\n",
        "          import uuid\n",
        "          COLAB_HTML_ROOT = \"/usr/local/share/jupyter/nbextensions/google.colab/\"\n",
        "          def moveToExt(filename:str) -> str:\n",
        "            if not os.path.exists(filename):\n",
        "              print(\"Image file not found\")\n",
        "              return None\n",
        "            \n",
        "            target = os.path.basename(filename)\n",
        "            target = os.path.join(COLAB_HTML_ROOT, str(uuid.uuid4()) + target)\n",
        "            \n",
        "            shutil.copyfile(filename,target)\n",
        "            print(\"moved to ext\")\n",
        "            return target\n",
        "          real_filename = os.path.realpath(filename)\n",
        "          html_filename = real_filename\n",
        "          html_real_filename = html_filename\n",
        "          if os.path.exists(real_filename):\n",
        "            html_real_filename = moveToExt(real_filename)\n",
        "            html_filename = html_real_filename.replace(\"/usr/local/share/jupyter\",\"\")\n",
        "            \n",
        "\n",
        "          canvas_html = f\"\"\"\n",
        "        <canvas width={w} height={h}></canvas>\n",
        "        <div>\n",
        "          <label for=\"strokeColor\">Stroke</label>\n",
        "          <input type=\"color\" value=\"{color}\" id=\"strokeColor\">\n",
        "        \n",
        "          <label for=\"bgColor\">Background</label>\n",
        "          <input type=\"color\" value=\"{bg_color}\" id=\"bgColor\">\n",
        "        </div>\n",
        "        <div class=\"slidecontainer\">\n",
        "        <label for=\"lineWidth\" id=\"lineWidthLabel\">{line_width}px</label>\n",
        "          <input type=\"range\" min=\"1\" max=\"35\" value=\"1\" class=\"slider\" id=\"lineWidth\">\n",
        "        </div>\n",
        "\n",
        "        <div>\n",
        "          <button id=\"loadImage\">Reload from disk</button>\n",
        "          <button id=\"reset\">Reset</button>\n",
        "          <button id=\"save\">Save</button>\n",
        "          <button id=\"exit\">Exit</button>\n",
        "        </div>\n",
        "        <script>\n",
        "\n",
        "        function loadImage(url) {{\n",
        "        return new Promise(r => {{ let i = new Image(); i.onload = (() => r(i)); i.src = url; }});\n",
        "      }}\n",
        "          \n",
        "          \n",
        "          var canvas = document.querySelector('canvas')\n",
        "          var ctx = canvas.getContext('2d')\n",
        "          ctx.lineWidth = {line_width}\n",
        "          ctx.fillStyle = \"{bg_color}\";\n",
        "          \n",
        "          ctx.fillRect(0, 0, canvas.width, canvas.height);\n",
        "          ctx.strokeStyle = \"{color}\";\n",
        "\n",
        "          var strokeColor = document.querySelector('#strokeColor')\n",
        "          var bgColor = document.querySelector('#bgColor')\n",
        "\n",
        "          var slider = document.getElementById(\"lineWidth\");\n",
        "          slider.oninput = function() {{\n",
        "            ctx.lineWidth = this.value;\n",
        "            lineWidthLabel.innerHTML = `${{this.value}}px`\n",
        "          }}\n",
        "\n",
        "          function updateStroke(event){{\n",
        "              ctx.strokeStyle = event.target.value\n",
        "          }}\n",
        "          function updateBG(event){{\n",
        "              ctx.fillStyle = event.target.value\n",
        "          }}\n",
        "          \n",
        "          bgColor.addEventListener(\"change\", updateBG, false);\n",
        "          strokeColor.addEventListener(\"change\", updateStroke, false);\n",
        "          \n",
        "          var clear_button = document.querySelector('#reset')\n",
        "          var reload_img_button = document.querySelector('#loadImage')\n",
        "          var button = document.querySelector('#save')\n",
        "          var exit_button = document.querySelector('#exit')\n",
        "\n",
        "          var mouse = {{x: 0, y: 0}}\n",
        "          canvas.addEventListener('mousemove', function(e) {{\n",
        "            mouse.x = e.pageX - this.offsetLeft\n",
        "            mouse.y = e.pageY - this.offsetTop\n",
        "          }})\n",
        "          canvas.onmousedown = ()=>{{\n",
        "            ctx.beginPath()\n",
        "            ctx.moveTo(mouse.x, mouse.y)\n",
        "            canvas.addEventListener('mousemove', onPaint)\n",
        "          }}\n",
        "          canvas.onmouseup = ()=>{{\n",
        "            canvas.removeEventListener('mousemove', onPaint)\n",
        "          }}\n",
        "          var onPaint = ()=>{{\n",
        "            ctx.lineTo(mouse.x, mouse.y)\n",
        "            ctx.stroke()\n",
        "          }}\n",
        "          reload_img_button.onclick = async ()=>{{\n",
        "            console.log(\"Reloading Image {html_filename}\")\n",
        "            let img = await loadImage('{html_filename}'); \n",
        "            console.log(\"Loaded image\")\n",
        "            ctx.drawImage(img, 0, 0);\n",
        "\n",
        "          }}\n",
        "          \n",
        "          clear_button.onclick = ()=>{{\n",
        "              console.log('Clearing Screen')\n",
        "              ctx.clearRect(0, 0, canvas.width, canvas.height);\n",
        "              ctx.fillRect(0, 0, canvas.width, canvas.height);\n",
        "            }}\n",
        "            canvas.addEventListener('load', function() {{\n",
        "            console.log('All assets are loaded')\n",
        "          }})\n",
        "          var data = new Promise(resolve=>{{\n",
        "            button.onclick = ()=>{{\n",
        "              resolve(canvas.toDataURL('image/png'))\n",
        "            }}\n",
        "            exit_button.onclick = ()=>{{\n",
        "            resolve()\n",
        "          }}\n",
        "            \n",
        "          }})\n",
        "          \n",
        "          // window.onload = async ()=>{{\n",
        "          //   console.log(\"loaded\")\n",
        "          //   let img = await loadImage('{html_filename}');  \n",
        "          //   ctx.drawImage(img, 0, 0);\n",
        "          // }}\n",
        "          \n",
        "          \n",
        "        </script>\n",
        "        \"\"\"\n",
        "          print(HTML)\n",
        "          display(HTML(canvas_html))\n",
        "          print(\"Evaluating JS\")\n",
        "          \n",
        "          data = google.colab.output.eval_js(\"data\")\n",
        "          if data:\n",
        "            print(\"Saving Sketch\")  \n",
        "            binary = b64decode(data.split(',')[1])\n",
        "            # filename = html_real_filename if loop else filename\n",
        "            with open(filename, 'wb') as f:\n",
        "              f.write(binary)\n",
        "            #return len(binary)\n",
        "        \n",
        "        draw(filename = \"custom_image.png\", w=width, h=height, bg_color=\"blue\", line_width=10)\n",
        "        import PIL.Image\n",
        "        return PIL.Image.open(\"/content/custom_image.png\")\n",
        "\n",
        "  class UserSettings:\n",
        "\n",
        "    def set_settings():\n",
        "      MODE = \"PROMPT\" #@param [\"PROMPT\", \"CLIP GUIDED PROMPT\", \"IMG2IMG\",\"Inpainting\",\"PROMPT FILE\"]\n",
        "      #@markdown `MODE` Select what mode you want to use <br>\n",
        "\n",
        "      #@markdown ---\n",
        "      settings = {\"mode\":MODE}\n",
        "      #@markdown GENERAL SETTINGS\n",
        "\n",
        "      WIDTH = 512 #@param {type:\"slider\", min:256, max:4096, step:64}\n",
        "      HEIGHT = 512 #@param {type:\"slider\", min:256, max:4096, step:64}\n",
        "      SCALE = 13.8 #@param {type:\"slider\", min:0, max:25, step:0.1}\n",
        "      SEED = 0 #@param {type:'integer'}\n",
        "      settings[\"seed\"] = SEED \n",
        "      SCHEDULER = 'default' #@param [\"default\", \"pndm\", \"k-lms\", \"ddim\", 'ddim clip sampled']\n",
        "      settings[\"scheduler\"] = SCHEDULER \n",
        "\n",
        "      if SCHEDULER == 'ddim':\n",
        "        DDIM_ETA = 0.72 #@param {type:\"slider\", min:0, max:1, step:0.01} \n",
        "        settings[\"ddim_eta\"] = DDIM_ETA \n",
        "      PRECISION = \"autocast\" #@param [\"full\",\"autocast\"]\n",
        "      settings[\"precision\"] = PRECISION\n",
        "      settings[\"width\"] = WIDTH\n",
        "      settings[\"height\"] = HEIGHT\n",
        "      settings[\"scale\"] = SCALE\n",
        "      IMG2IMG_POSTPROCESS = False #@param {type:'boolean'}\n",
        "      #@markdown `IMG2IMG_POSTPROCESS`: Postprocess the image with img2img after it's done. Will take the img2img settings to do the postprocessing, so make sure to change those if you set this. It can add a lot of detail to the final image after upscaling (sometimes it's hit or miss) although it is very slow since it needs to switch pipes<br>CAUTION: Very error-prone (will fix that down the road). If this runs into an error you need to clean the environment by clicking on \"Runtime\" and then on \"Restart Environment\"\n",
        "      settings[\"img2img_postprocess\"] = IMG2IMG_POSTPROCESS\n",
        "      \n",
        "      #@markdown ---\n",
        "\n",
        "      #@markdown UPSCALING SETTINGS\n",
        "\n",
        "      IMAGE_UPSCALER = \"GFPGAN + Enhanced ESRGAN\" #@param [\"None\",\"GFPGAN\",\"Enhanced Real-ESRGAN\", \"GFPGAN + Enhanced ESRGAN\", \"CodeFormer\", \"CodeFormer + Enhanced ESRGAN\", \"IMG2IMG\"]\n",
        "      settings[\"image_upscaler\"] = IMAGE_UPSCALER \n",
        "\n",
        "      UPSCALE_AMOUNT = 2 #@param {type:\"raw\"}\n",
        "      settings[\"upscale_amount\"] = UPSCALE_AMOUNT \n",
        "\n",
        "      FIDELITY = 0.8 #@param {type:\"slider\", min:0, max:1, step:0.01}\n",
        "      settings[\"codeformer_fidelity\"] = FIDELITY\n",
        "\n",
        "      SHARPEN_AMOUNT = 1 #@param{type:'slider', min:0, max:3, step:1}\n",
        "      settings[\"sharpen_amount\"] = SHARPEN_AMOUNT\n",
        "\n",
        "      \n",
        "      #@markdown ---\n",
        "\n",
        "      #@markdown MODE: PROMPT FILE SETTINGS\n",
        "      if MODE == \"PROMPT FILE\":\n",
        "        FILE_LOCATION = \"/content/diffusers_output/1663720628_prompt.json\" #@param {type:\"string\"}\n",
        "        settings['prompt_file'] = FILE_LOCATION\n",
        "\n",
        "      #@markdown ---\n",
        "\n",
        "      if MODE == \"PROMPT\":\n",
        "        #@markdown MODE: PROMPT SETTINGS\n",
        "        # PROMPT_TYPE = \"TEXT\" #@param [\"TEXT\",\"FILE\"]\n",
        "        # TODO\n",
        "        settings[\"prompt_type\"] = \"TEXT\"\n",
        "\n",
        "        if settings[\"prompt_type\"] == \"TEXT\":\n",
        "          TEXT_PROMPT = \"A young woman wearing a hat, greg rutkowski, artgerm, trending on artstation, cinematic animation still, by lois van baarle, ilya kuvshinov, metahuman\" #@param {type:\"string\"}\n",
        "          settings[\"text_prompt\"] = TEXT_PROMPT\n",
        "        # elif PROMPT_TYPE == \"FILE\":\n",
        "        #   FILE_PROMPT = \"/content/prompt_file.txt\" #@param {type:\"string\"}\n",
        "        #   prompts = []\n",
        "        #   with open(FILE_PROMPT, 'r') as file:\n",
        "        #     for line in file.readlines():\n",
        "        #       prompts.append(line)\n",
        "        #   settings[\"file_prompt\"] = prompts\n",
        "        # TODO\n",
        "\n",
        "        PROMPT_STEPS = 200 #@param {type:\"slider\", min:5, max:500, step:5} \n",
        "        settings[\"steps\"] = PROMPT_STEPS \n",
        "\n",
        "      #@markdown ---\n",
        "\n",
        "      elif MODE == \"CLIP GUIDED PROMPT\":\n",
        "        \n",
        "        #@markdown MODE: CLIP GUIDED PROMPT (still buggy and finicky, but works with autocast & low_vram_patch)<br>\n",
        "        #@markdown The VRAM tends to stick if this errors out. When that happens click on \"Runtime\" and then \"Restart Runtime\". It's very finicky, some settings work better than others.\n",
        "        CLIP_MODEL_ID = \"laion/CLIP-ViT-B-32-laion2B-s34B-b79K\" #@param [\"laion/CLIP-ViT-B-32-laion2B-s34B-b79K\", \"laion/CLIP-ViT-L-14-laion2B-s32B-b82K\", \"laion/CLIP-ViT-H-14-laion2B-s32B-b79K\", \"laion/CLIP-ViT-g-14-laion2B-s12B-b42K\", \"openai/clip-vit-base-patch32\", \"openai/clip-vit-base-patch16\", \"openai/clip-vit-large-patch14\"] {allow-input: true}\n",
        "        settings[\"clip_model_id\"] = CLIP_MODEL_ID\n",
        "        CLIP_MODE_TEXT_PROMPT = \"A young woman wearing a hat, greg rutkowski, artgerm, trending on artstation, cinematic animation still, by lois van baarle, ilya kuvshinov, metahuman\" #@param {type:\"string\"}\n",
        "        settings[\"text_prompt\"] = CLIP_MODE_TEXT_PROMPT\n",
        "        CLIP_GUIDANCE_PROMPT = \"\" #@param {type:\"string\"}\n",
        "        settings[\"clip_prompt\"] = CLIP_GUIDANCE_PROMPT\n",
        "        CLIP_MODE_STEPS = 200 #@param {type:\"integer\"}\n",
        "        settings[\"steps\"] = CLIP_MODE_STEPS\n",
        "        CLIP_MODE_SCALE = 13.7 #@param {type:\"raw\"}\n",
        "        settings[\"scale\"] = CLIP_MODE_SCALE\n",
        "        CLIP_GUIDANCE_SCALE = 100 #@param {type:\"raw\"}\n",
        "        settings[\"clip_guidance_scale\"] = CLIP_GUIDANCE_SCALE\n",
        "        CLIP_MODE_NUM_CUTOUTS = 64 #@param {type:\"raw\"}\n",
        "        settings[\"clip_cutouts\"] = CLIP_MODE_NUM_CUTOUTS\n",
        "        CLIP_UNFREEZE_UNET = True #@param {type:\"boolean\"}\n",
        "        settings[\"unfreeze_unet\"] = CLIP_UNFREEZE_UNET\n",
        "        CLIP_UNFREEZE_VAE = True #@param {type:\"boolean\"}\n",
        "        settings[\"unfreeze_vae\"] = CLIP_UNFREEZE_VAE\n",
        "\n",
        "    \n",
        "      elif MODE == \"Inpainting\":\n",
        "        #@markdown ---\n",
        "        \n",
        "        #@markdown MODE: Inpainting SETTINGS\n",
        "        INPAINT_PROMPT = \"A cat sitting on a bench\" #@param {type:\"string\"}\n",
        "        settings[\"text_prompt\"] = INPAINT_PROMPT\n",
        "        \n",
        "        INPAINT_IMAGE = \"https://raw.githubusercontent.com/CompVis/latent-diffusion/main/data/inpainting_examples/overture-creations-5sI6fQgYIuo.png\" #@param {type:'string'}\n",
        "        settings[\"inpaint_image\"] = INPAINT_IMAGE\n",
        "        \n",
        "        MASK_IMAGE = \"\" #@param {type:'string'}\n",
        "        settings[\"mask_image\"] = MASK_IMAGE\n",
        "        \n",
        "        INPAINT_STRENGTH = 0.5 #@param {type:\"slider\", min:0, max:1, step:0.01} \n",
        "        settings[\"inpaint_strength\"] = INPAINT_STRENGTH\n",
        "\n",
        "        INPAINT_MODE_STEPS = 200 #@param {type:\"slider\", min:5, max:500, step:5} \n",
        "        settings[\"steps\"] = INPAINT_MODE_STEPS\n",
        "\n",
        "\n",
        "      if MODE == \"IMG2IMG\" or settings[\"img2img_postprocess\"] or settings['image_upscaler'] == 'IMG2IMG':\n",
        "        #@markdown ---\n",
        "        \n",
        "        #@markdown MODE: IMG2IMG SETTINGS\n",
        "        IMG_PROMPT = \"A young woman wearing a hat, greg rutkowski, artgerm, trending on artstation, cinematic animation still, by lois van baarle, ilya kuvshinov, metahuman\" #@param {type:\"string\"}\n",
        "        \n",
        "        INIT_IMAGE = \"https://raw.githubusercontent.com/dblunk88/txt2imghd/master/character_with_hat.jpg\" #@param {type: 'string'}\n",
        "        settings[\"init_image\"] = INIT_IMAGE\n",
        "        \n",
        "        INIT_STRENGTH = 0.6 #@param{type:\"slider\", min:0.01, max:1, step:0.01}\n",
        "        IMG2IMG_MODE_STEPS=40 #@param {type:\"slider\", min:5, max:500, step:5} \n",
        "        \n",
        "        if settings[\"img2img_postprocess\"] or settings['image_upscaler'] == 'IMG2IMG':\n",
        "          if settings['mode'] == \"IMG2IMG\":\n",
        "            settings[\"text_prompt\"] = IMG_PROMPT\n",
        "            settings[\"init_strength\"] = INIT_STRENGTH\n",
        "            settings[\"steps\"] = IMG2IMG_MODE_STEPS\n",
        "          settings['img2img'] = {}\n",
        "          settings['img2img'][\"text_prompt\"] = IMG_PROMPT\n",
        "          settings['img2img'][\"init_strength\"] = INIT_STRENGTH\n",
        "          settings['img2img'][\"steps\"] = IMG2IMG_MODE_STEPS \n",
        "        else:\n",
        "          settings[\"text_prompt\"] = IMG_PROMPT\n",
        "          settings[\"init_strength\"] = INIT_STRENGTH\n",
        "          settings[\"steps\"] = IMG2IMG_MODE_STEPS \n",
        "\n",
        "\n",
        "      #@markdown ---\n",
        "      \n",
        "\n",
        "      #@markdown Version Settings\n",
        "      #https://github.com/huggingface/diffusers/blob/main/scripts/convert_original_stable_diffusion_to_diffusers.py\n",
        "      MODEL_ID = \"CompVis/stable-diffusion-v1-4\" #@param [\"CompVis/stable-diffusion-v1-4\", \"CompVis/stable-diffusion-v1-3\",\"CompVis/stable-diffusion-v1-2\",\"CompVis/stable-diffusion-v1-1\",'hakurei/waifu-diffusion', 'ayan4m1/trinart_diffusers_v2', 'doohickey/trinart-waifu-diffusion-50-50', 'lambdalabs/sd-pokemon-diffusers', 'anton-l/ddpm-ema-pokemon-64'] {allow-input:true}\n",
        "      settings[\"model_id\"] = MODEL_ID\n",
        "\n",
        "      DIFFUSERS_VERSION = '91db81894b44798649b6cf54be085c205e146805' #@param [\"latest\", \"91db81894b44798649b6cf54be085c205e146805\", \"f3937bc8f3667772c9f1428b66f0c44b6087b04d\"]\n",
        "      settings[\"diffusers_version\"] = DIFFUSERS_VERSION\n",
        "      \n",
        "      #@markdown ---\n",
        "      \n",
        "      #@markdown ADVANCED SETTINGS\n",
        "      CLEAN_PREVIEW_AFTER_ITERS = 19 #@param {type:\"slider\", min:1, max:100, step:1} \n",
        "      settings[\"clean_iters\"] = CLEAN_PREVIEW_AFTER_ITERS\n",
        "\n",
        "      SKIP_BULKY_PREVIEWS = False #@param {type:'boolean'}\n",
        "      settings[\"bulky_skip\"] = SKIP_BULKY_PREVIEWS\n",
        "\n",
        "      KEEP_SEED = False #@param {type:'boolean'}\n",
        "      settings[\"keep_seed\"] = KEEP_SEED\n",
        "\n",
        "      NUM_ITERS = 4 #@param {type:\"slider\", min:1, max:100, step:1} \n",
        "      settings[\"num_iters\"] = NUM_ITERS\n",
        "\n",
        "      RUN_FOREVER = False #@param {type:\"boolean\"}\n",
        "      settings[\"run_forever\"] = RUN_FOREVER\n",
        "\n",
        "      SAVE_PROMPT_DETAILS = True #@param {type:\"boolean\"}\n",
        "      settings[\"save_prompt_details\"] = SAVE_PROMPT_DETAILS\n",
        "\n",
        "      USE_DRIVE_FOR_PICS = True #@param {type:\"boolean\"}\n",
        "      settings[\"use_drive_for_pics\"] = USE_DRIVE_FOR_PICS\n",
        "\n",
        "      \n",
        "\n",
        "      if USE_DRIVE_FOR_PICS:\n",
        "        DRIVE_PIC_DIR = \"AI_PICS\" #@param {type:\"string\"}\n",
        "        settings[\"drive_pic_dir\"] = DRIVE_PIC_DIR\n",
        "\n",
        "      DELETE_ORIGINALS = True #@param{type:'boolean'}\n",
        "      settings[\"delete_originals\"] = DELETE_ORIGINALS\n",
        "\n",
        "      LOW_VRAM_PATCH = True #@param {type:\"boolean\"}\n",
        "      settings[\"low_vram_patch\"] = LOW_VRAM_PATCH\n",
        "\n",
        "      VRAM_OVER_SPEED = True #@param {type:\"boolean\"}\n",
        "      settings[\"vram_over_speed\"] = VRAM_OVER_SPEED\n",
        "\n",
        "      ENABLE_NSFW_FILTER = False #@param {type:\"boolean\"}\n",
        "      settings[\"enable_nsfw_filter\"] = ENABLE_NSFW_FILTER\n",
        "\n",
        "  \n",
        "\n",
        "      return settings\n",
        "\n",
        "class Upscalers:\n",
        "\n",
        "  def check_upscalers(settings,image):\n",
        "    Cleaner.clean_env()\n",
        "    if settings['image_upscaler'] == 'GFPGAN':\n",
        "      image = Upscalers.gfpgan(settings, image)\n",
        "    elif settings['image_upscaler'] == 'Enhanced Real-ESRGAN':\n",
        "      image = Upscalers.esrgan(settings, image)\n",
        "    elif settings['image_upscaler'] == 'GFPGAN + Enhanced ESRGAN':\n",
        "      image = Upscalers.gfpgan_esrgan(settings, image)\n",
        "    elif settings['image_upscaler'] == 'CodeFormer':\n",
        "      image = Upscalers.codeformer(settings, image)\n",
        "    elif settings['image_upscaler'] == 'CodeFormer + Enhanced ESRGAN':\n",
        "      image = Upscalers.codeformer_esrgan(settings, image)\n",
        "    Cleaner.clean_env()\n",
        "    return image\n",
        "\n",
        "  def gfpgan(settings, image):\n",
        "    import os, subprocess\n",
        "    if not os.path.exists('/content/GFPGAN/'):\n",
        "      Upscalers.Install.gfpgan()\n",
        "    os.chdir('/content/GFPGAN/')\n",
        "    print(subprocess.run(['mkdir', f'/content/GFPGAN/results/'], stdout=subprocess.PIPE).stdout.decode('utf-8'))\n",
        "    print(subprocess.run(['mkdir', f'/content/GFPGAN/temp/'], stdout=subprocess.PIPE).stdout.decode('utf-8'))\n",
        "    image.save('/content/GFPGAN/temp/temp.png')\n",
        "    print(subprocess.run(['python',f'/content/GFPGAN/inference_gfpgan.py','-i', '/content/GFPGAN/temp/temp.png','-o','/content/GFPGAN/results/', '-w', f'{settings[\"codeformer_fidelity\"]}','-s',f'{settings[\"upscale_amount\"]}', '--bg_upsampler', 'realesrgan'], stdout=subprocess.PIPE).stdout.decode('utf-8'))\n",
        "    import PIL.Image\n",
        "    image = PIL.Image.open(f'/content/GFPGAN/results/restored_imgs/temp.png')\n",
        "    print(subprocess.run(['rm', '-rf', f'/content/GFPGAN/results/'], stdout=subprocess.PIPE).stdout.decode('utf-8'))\n",
        "    print(subprocess.run(['rm', '-rf', f'/content/GFPGAN/temp/'], stdout=subprocess.PIPE).stdout.decode('utf-8'))\n",
        "    os.chdir('/content/')\n",
        "    return image\n",
        "\n",
        "  def esrgan(settings, image):\n",
        "    def closest_value(input_list, input_value):\n",
        "      difference = lambda input_list : abs(input_list - input_value)\n",
        "      res = min(input_list, key=difference)\n",
        "      return res\n",
        "    import os\n",
        "    if int(settings[\"upscale_amount\"]) not in [1,2,4,8]:\n",
        "      nearest_value = closest_value([2,4,8],settings[\"upscale_amount\"])\n",
        "      settings[\"upscale_amount\"] = nearest_value\n",
        "      print(f'For Real-ESRGAN upscaling only 2, 4, and 8 are supported. Choosing the nearest Value: {nearest_value}')\n",
        "    if not os.path.exists('/content/Real-ESRGAN'):\n",
        "      Upscalers.Install.esrgan()\n",
        "    if not os.path.exists(f'/content/Real-ESRGAN/weights/RealESRGAN_x{settings[\"upscale_amount\"]}.pth'):\n",
        "      import subprocess\n",
        "      print(subprocess.run(['wget',f'https://huggingface.co/datasets/db88/Enhanced_ESRGAN/resolve/main/RealESRGAN_x{settings[\"upscale_amount\"]}.pth','-O',f'/content/Real-ESRGAN/weights/RealESRGAN_x{settings[\"upscale_amount\"]}.pth'], stdout=subprocess.PIPE).stdout.decode('utf-8'))\n",
        "    os.chdir('/content/Real-ESRGAN')\n",
        "    from realesrgan import RealESRGAN\n",
        "    import torch\n",
        "    model = RealESRGAN(torch.device('cuda'), scale = settings[\"upscale_amount\"])\n",
        "    model.load_weights(f'/content/Real-ESRGAN/weights/RealESRGAN_x{settings[\"upscale_amount\"]}.pth')\n",
        "    import numpy as np\n",
        "    image = model.predict(np.array(image))\n",
        "    os.chdir('/content/')\n",
        "    model = None\n",
        "    Cleaner.clean_env()\n",
        "    return image\n",
        "\n",
        "  def codeformer(settings, image):\n",
        "    import os, subprocess\n",
        "    if not os.path.exists('/content/CodeFormer/'):\n",
        "      Upscalers.Install.codeformer()\n",
        "    print(subprocess.run(['mkdir', f'/content/CodeFormer/results/'], stdout=subprocess.PIPE).stdout.decode('utf-8'))\n",
        "    print(subprocess.run(['mkdir', f'/content/CodeFormer/temp/'], stdout=subprocess.PIPE).stdout.decode('utf-8'))\n",
        "    image.save('/content/CodeFormer/temp/temp.png')\n",
        "    os.chdir('/content/CodeFormer/')\n",
        "    print(subprocess.run(['python',f'inference_codeformer.py','--w', f'{settings[\"codeformer_fidelity\"]}','--test_path','/content/CodeFormer/temp','--upscale',f'{settings[\"upscale_amount\"]}', '--bg_upsampler', 'realesrgan'], stdout=subprocess.PIPE).stdout.decode('utf-8'))\n",
        "    import PIL.Image\n",
        "    image = PIL.Image.open(f'/content/CodeFormer/results/temp_{settings[\"codeformer_fidelity\"]}/final_results/temp.png')\n",
        "    print(subprocess.run(['rm', '-rf', f'/content/CodeFormer/results/'], stdout=subprocess.PIPE).stdout.decode('utf-8'))\n",
        "    print(subprocess.run(['rm', '-rf', f'/content/CodeFormer/temp/'], stdout=subprocess.PIPE).stdout.decode('utf-8'))\n",
        "    os.chdir('/content/')\n",
        "    return image\n",
        "\n",
        "  def gfpgan_esrgan(settings, image):\n",
        "    orig_upscale = settings['upscale_amount']\n",
        "    settings['upscale_amount'] = 1\n",
        "    image = Upscalers.gfpgan(settings, image)\n",
        "    settings['upscale_amount'] = orig_upscale\n",
        "    image = Upscalers.esrgan(settings, image)\n",
        "    return image\n",
        "\n",
        "  def codeformer_esrgan(settings, image):\n",
        "    orig_upscale = settings['upscale_amount']\n",
        "    settings['upscale_amount'] = 1\n",
        "    image = Upscalers.codeformer(settings, image)\n",
        "    settings['upscale_amount'] = orig_upscale\n",
        "    image = Upscalers.esrgan(settings, image)\n",
        "    return image\n",
        "    \n",
        "\n",
        "  class Install:\n",
        "\n",
        "    def gfpgan():\n",
        "      import subprocess\n",
        "      print(subprocess.run(['git','clone','https://github.com/TencentARC/GFPGAN.git'], stdout=subprocess.PIPE).stdout.decode('utf-8'))\n",
        "      print(subprocess.run(['pip','install','basicsr'], stdout=subprocess.PIPE).stdout.decode('utf-8'))\n",
        "      print(subprocess.run(['pip','install','facexlib'], stdout=subprocess.PIPE).stdout.decode('utf-8'))\n",
        "      import os\n",
        "      os.chdir('/content/GFPGAN')\n",
        "      print(subprocess.run(['pip','install', '-r', 'requirements.txt'], stdout=subprocess.PIPE).stdout.decode('utf-8'))\n",
        "      print(subprocess.run(['python','setup.py', 'develop'], stdout=subprocess.PIPE).stdout.decode('utf-8'))\n",
        "      print(subprocess.run(['wget','https://github.com/TencentARC/GFPGAN/releases/download/v1.3.0/GFPGANv1.3.pth', '-P', '/content/GFPGAN/experiments/pretrained_models'], stdout=subprocess.PIPE).stdout.decode('utf-8'))\n",
        "      print(subprocess.run(['pip','install','realesrgan'], stdout=subprocess.PIPE).stdout.decode('utf-8'))\n",
        "      os.chdir('/content/')\n",
        "\n",
        "    def esrgan():\n",
        "      import subprocess, os\n",
        "      print(subprocess.run(['git','clone','https://github.com/sberbank-ai/Real-ESRGAN'], stdout=subprocess.PIPE).stdout.decode('utf-8'))\n",
        "      os.chdir('Real-ESRGAN')\n",
        "      print(subprocess.run(['git','reset', '--hard','2a5afd04a0e43956d1640db00d3a528ca5972fd2'], stdout=subprocess.PIPE).stdout.decode('utf-8'))\n",
        "      print(subprocess.run(['pip','install','-r','/content/Real-ESRGAN/requirements.txt'], stdout=subprocess.PIPE).stdout.decode('utf-8'))\n",
        "      os.chdir('/content/')\n",
        "      \n",
        "    def codeformer():\n",
        "      import subprocess\n",
        "      import os\n",
        "      print(subprocess.run(['git','clone','https://github.com/sczhou/CodeFormer.git'], stdout=subprocess.PIPE).stdout.decode('utf-8'))\n",
        "      print(subprocess.run(['pip','install','-r','/content/CodeFormer/requirements.txt'], stdout=subprocess.PIPE).stdout.decode('utf-8'))\n",
        "      os.chdir('/content/CodeFormer/')\n",
        "      print(subprocess.run(['python','basicsr/setup.py','develop'], stdout=subprocess.PIPE).stdout.decode('utf-8'))\n",
        "      print(subprocess.run(['python','scripts/download_pretrained_models.py','facelib'], stdout=subprocess.PIPE).stdout.decode('utf-8'))\n",
        "      print(subprocess.run(['python','scripts/download_pretrained_models.py','CodeFormer'], stdout=subprocess.PIPE).stdout.decode('utf-8'))\n",
        "      os.chdir('/content/')\n",
        "\n",
        "class Cache:\n",
        "  class Pipe:\n",
        "    def __init__(self, settings):\n",
        "      global pipe\n",
        "      global pipetype\n",
        "      try:\n",
        "        if pipetype != settings[\"mode\"] or pipe is None:\n",
        "          self.make(settings)\n",
        "      except NameError:\n",
        "        self.make(settings)\n",
        "      Manager.Diffusion.Scheduler.make(settings)\n",
        "      self.pipe = pipe\n",
        "      self.pipetype = settings_pipetype\n",
        "\n",
        "    def forward(self, x, context=None, mask=None):\n",
        "\n",
        "      import math\n",
        "      from torch import einsum\n",
        "      try:\n",
        "        from einops import rearrange\n",
        "      except ModuleNotFoundError:\n",
        "        !pip install einops\n",
        "        from einops import rearrange\n",
        "      import types\n",
        "      from diffusers.models.attention import CrossAttention\n",
        "      import torch\n",
        "      batch_size, sequence_length, dim = x.shape\n",
        "\n",
        "      h = self.heads\n",
        "\n",
        "      q = self.to_q(x)\n",
        "      context = context if context is not None else x\n",
        "      k = self.to_k(context)\n",
        "      v = self.to_v(context)\n",
        "      del context, x\n",
        "\n",
        "      q = self.reshape_heads_to_batch_dim(q)\n",
        "      k = self.reshape_heads_to_batch_dim(k)\n",
        "      v = self.reshape_heads_to_batch_dim(v)\n",
        "\n",
        "      r1 = torch.zeros(q.shape[0], q.shape[1], v.shape[2], device=q.device)\n",
        "\n",
        "      stats = torch.cuda.memory_stats(q.device)\n",
        "      mem_total = torch.cuda.get_device_properties(0).total_memory\n",
        "      mem_active = stats['active_bytes.all.current']\n",
        "      mem_free = mem_total - mem_active\n",
        "\n",
        "      mem_required = q.shape[0] * q.shape[1] * k.shape[1] * 4 * 2.5\n",
        "      steps = 1\n",
        "\n",
        "      if mem_required > mem_free:\n",
        "          steps = 2**(math.ceil(math.log(mem_required / mem_free, 2)))\n",
        "\n",
        "      slice_size = q.shape[1] // steps if (q.shape[1] % steps) == 0 else q.shape[1]\n",
        "      for i in range(0, q.shape[1], slice_size):\n",
        "          end = i + slice_size\n",
        "          s1 = einsum('b i d, b j d -> b i j', q[:, i:end], k)\n",
        "          s1 *= self.scale\n",
        "\n",
        "          s2 = s1.softmax(dim=-1)\n",
        "          del s1\n",
        "\n",
        "          r1[:, i:end] = einsum('b i j, b j d -> b i d', s2, v)\n",
        "          del s2\n",
        "\n",
        "      del q, k, v\n",
        "\n",
        "      r2 = rearrange(r1, '(b h) n d -> b n (h d)', h=h)\n",
        "      del r1\n",
        "\n",
        "      return self.to_out(r2)\n",
        "\n",
        "    def optimize_attention(model):\n",
        "        import types\n",
        "        from diffusers.models.attention import CrossAttention\n",
        "        for module in model.modules():\n",
        "            if isinstance(module, CrossAttention):\n",
        "                module.forward = types.MethodType(Cache.Pipe.forward, module)\n",
        "\n",
        "    def make(settings):\n",
        "      global pipe \n",
        "      global pipetype\n",
        "      pipe = None\n",
        "      Cleaner.clean_env()\n",
        "      pipetype = settings['mode']\n",
        "      pipe_library = Manager.manage_imports(pipetype)\n",
        "      import os, subprocess, torch\n",
        "      username, token = Manager.Diffusion.creds()\n",
        "      subprocess.run(['git', 'config', '--global', 'credential.helper', 'store'], stdout=subprocess.DEVNULL)\n",
        "      left_of_pipe = subprocess.Popen([\"echo\", token], stdout=subprocess.PIPE)\n",
        "      right_of_pipe = subprocess.run(['huggingface-cli', 'login'], stdin=left_of_pipe.stdout, stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
        "      print('Making Pipe')\n",
        "      if settings['mode'] == \"CLIP GUIDED PROMPT\":\n",
        "        import torch\n",
        "        from diffusers import LMSDiscreteScheduler, StableDiffusionPipeline\n",
        "        from PIL import Image\n",
        "        from transformers import CLIPFeatureExtractor, CLIPModel\n",
        "        import CLIP_GUIDED\n",
        "        def create_clip_guided_pipeline(\n",
        "            model_id=\"CompVis/stable-diffusion-v1-4\", clip_model_id=\"laion/CLIP-ViT-B-32-laion2B-s34B-b79K\", scheduler=\"plms\", low_vram_patch=True\n",
        "        ):\n",
        "            if low_vram_patch:\n",
        "              pipeline = StableDiffusionPipeline.from_pretrained(\n",
        "                  model_id,\n",
        "                  torch_dtype=torch.float16,\n",
        "                  revision=\"fp16\",\n",
        "                  use_auth_token=True,\n",
        "              )\n",
        "              clip_model = CLIPModel.from_pretrained(clip_model_id, torch_dtype=torch.float16)\n",
        "              feature_extractor = CLIPFeatureExtractor.from_pretrained(clip_model_id, torch_dtype=torch.float16)\n",
        "            else:\n",
        "              pipeline = StableDiffusionPipeline.from_pretrained(\n",
        "                  model_id,\n",
        "                  use_auth_token=True,\n",
        "              )\n",
        "              clip_model = CLIPModel.from_pretrained(clip_model_id)\n",
        "              feature_extractor = CLIPFeatureExtractor.from_pretrained(clip_model_id)\n",
        "\n",
        "            if settings[\"scheduler\"] == 'ddim clip sampled':\n",
        "              schedulers = Manager.manage_imports('ddim')\n",
        "            else:\n",
        "              schedulers = Manager.manage_imports(settings[\"scheduler\"])\n",
        "            if settings[\"scheduler\"] == 'default' or settings[\"scheduler\"] == 'pndm':\n",
        "              scheduler = schedulers(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\",\n",
        "                                               num_train_timesteps=1000, skip_prk_steps=True)\n",
        "            elif settings[\"scheduler\"] == 'k-lms':\n",
        "              scheduler = schedulers(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\",\n",
        "                                                      num_train_timesteps=1000)\n",
        "            elif settings[\"scheduler\"] == 'ddim':\n",
        "              scheduler = schedulers(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", clip_sample=False, set_alpha_to_one=False)\n",
        "            elif settings[\"scheduler\"] == 'ddim clip sampled':\n",
        "              scheduler = schedulers(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", clip_sample=True, set_alpha_to_one=False)\n",
        "            \n",
        "\n",
        "            guided_pipeline = CLIP_GUIDED.CLIPGuidedStableDiffusion(\n",
        "                unet=pipeline.unet,\n",
        "                vae=pipeline.vae,\n",
        "                tokenizer=pipeline.tokenizer,\n",
        "                text_encoder=pipeline.text_encoder,\n",
        "                scheduler=scheduler,\n",
        "                clip_model=clip_model,\n",
        "                feature_extractor=feature_extractor,\n",
        "            )\n",
        "\n",
        "            return guided_pipeline\n",
        "        \n",
        "        local_pipe = create_clip_guided_pipeline(settings[\"model_id\"], settings[\"clip_model_id\"], settings['scheduler'], settings[\"low_vram_patch\"])\n",
        "        if settings[\"vram_over_speed\"]:\n",
        "          local_pipe.enable_attention_slicing()\n",
        "          Cache.Pipe.optimize_attention(local_pipe.unet)\n",
        "        local_pipe = local_pipe.to(\"cuda\")\n",
        "      else:\n",
        "        if settings[\"low_vram_patch\"]:\n",
        "          try:\n",
        "            local_pipe = pipe_library.from_pretrained(settings['model_id'], revision=\"fp16\", torch_dtype=torch.float16, use_auth_token=True).to(\"cuda\")\n",
        "          except OSError:\n",
        "            local_pipe = pipe_library.from_pretrained(settings['model_id'], use_auth_token=True).to(\"cuda\")\n",
        "          if settings[\"mode\"] == \"PROMPT\":\n",
        "            del local_pipe.vae.encoder\n",
        "        else:\n",
        "          local_pipe = pipe_library.from_pretrained(settings['model_id'], use_auth_token=True).to(\"cuda\")\n",
        "        if settings[\"vram_over_speed\"]:\n",
        "          local_pipe.enable_attention_slicing()\n",
        "          Cache.Pipe.optimize_attention(local_pipe.unet)\n",
        "      pipe = local_pipe\n",
        "      local_pipe = None\n",
        "      Cleaner.clean_env()\n",
        "\n",
        "  \n",
        "\n",
        "class Manager:\n",
        "  def __init__(self):\n",
        "    self.colab = Colab()\n",
        "    self.cache = Cache()\n",
        "    from IPython.display import Javascript\n",
        "    display(Javascript(\"google.colab.output.resizeIframeToContent()\"))\n",
        "\n",
        "  def manage_imports(requester):\n",
        "    if requester == 'manage_drive':\n",
        "      from os.path import exists\n",
        "      from os import makedirs\n",
        "      from google.colab import drive\n",
        "      return exists, drive.mount, makedirs\n",
        "\n",
        "    elif requester == 'patch_nsfw':\n",
        "      from shutil import copyfile\n",
        "      from os import remove\n",
        "      return copyfile, remove\n",
        "\n",
        "    elif requester == 'general_diffusion_run':\n",
        "      import torch, sys\n",
        "      from random import randint\n",
        "      return torch, randint, sys\n",
        "\n",
        "    elif requester == \"prompt_run\":\n",
        "      pass\n",
        "\n",
        "    elif requester == \"img2img_run\":\n",
        "      pass\n",
        "    \n",
        "    elif requester == \"inpainter_run\":\n",
        "      pass\n",
        "    \n",
        "    elif requester == 'clean_env':\n",
        "      import gc, torch\n",
        "      return gc, torch\n",
        "\n",
        "    elif requester == \"PROMPT\":\n",
        "      try:\n",
        "        from diffusers import StableDiffusionPipeline\n",
        "      except ModuleNotFoundError:\n",
        "        Manager.Diffusion.install_diffusers()\n",
        "        from diffusers import StableDiffusionPipeline\n",
        "      return StableDiffusionPipeline\n",
        "    \n",
        "    elif requester == \"IMG2IMG\":\n",
        "      try:\n",
        "        from diffusers import StableDiffusionImg2ImgPipeline\n",
        "      except ModuleNotFoundError:\n",
        "        Manager.Diffusion.install_diffusers()\n",
        "        from diffusers import StableDiffusionImg2ImgPipeline\n",
        "      return StableDiffusionImg2ImgPipeline\n",
        "\n",
        "    elif requester == \"Inpainting\":\n",
        "      try:\n",
        "        from diffusers import StableDiffusionInpaintPipeline\n",
        "      except ModuleNotFoundError:\n",
        "        Manager.Diffusion.install_diffusers()\n",
        "        from diffusers import StableDiffusionInpaintPipeline\n",
        "      return StableDiffusionInpaintPipeline\n",
        "\n",
        "    elif requester == \"CLIP GUIDED PROMPT\":\n",
        "      try: \n",
        "        from CLIP_GUIDED import CLIPGuidedStableDiffusion\n",
        "      except ModuleNotFoundError:\n",
        "        Manager.Diffusion.install_diffusers()\n",
        "        from CLIP_GUIDED import CLIPGuidedStableDiffusion\n",
        "      return CLIPGuidedStableDiffusion\n",
        "\n",
        "    elif requester == 'diffuser_install':\n",
        "      import subprocess, os\n",
        "      return subprocess, os\n",
        "\n",
        "    elif requester == 'default' or requester == 'pndm':\n",
        "      from diffusers.schedulers import PNDMScheduler\n",
        "      return PNDMScheduler\n",
        "    \n",
        "    elif requester == 'k-lms':\n",
        "      from diffusers.schedulers import LMSDiscreteScheduler\n",
        "      return LMSDiscreteScheduler\n",
        "\n",
        "    elif requester == 'ddim':\n",
        "      from diffusers.schedulers import DDIMScheduler\n",
        "      return DDIMScheduler\n",
        "\n",
        "\n",
        "\n",
        "  def eval_settings(self):\n",
        "    settings = self.colab.settings\n",
        "    if settings['mode'] == \"PROMPT FILE\":\n",
        "      with open(settings['prompt_file'],'r') as file:\n",
        "        import json\n",
        "        self.colab.settings = json.loads(file.read())\n",
        "        settings = self.colab.settings\n",
        "    import json\n",
        "    print(json.dumps(settings, indent=2))\n",
        "    global pipetype\n",
        "    global pipe\n",
        "    try:\n",
        "      if pipetype != settings['mode'] or pipe is None:\n",
        "        Cache.Pipe.make(settings)\n",
        "    except NameError:\n",
        "      Cache.Pipe.make(settings)\n",
        "    if settings[\"use_drive_for_pics\"]:\n",
        "      Colab.manage_drive(settings['drive_pic_dir'])\n",
        "    \n",
        "\n",
        "  class Diffusion:\n",
        "    def patch_nsfw(ENABLE_NSFW_FILTER):\n",
        "      copyfile, remove = Manager.manage_imports('patch_nsfw')\n",
        "      remove('/usr/local/lib/python3.7/dist-packages/diffusers/pipelines/stable_diffusion/safety_checker.py')\n",
        "      if ENABLE_NSFW_FILTER:\n",
        "        copyfile(f'/content/safety_checker.py', '/usr/local/lib/python3.7/dist-packages/diffusers/pipelines/stable_diffusion/safety_checker.py')\n",
        "      else:\n",
        "        copyfile(f'/content/safety_checker_patched.py', '/usr/local/lib/python3.7/dist-packages/diffusers/pipelines/stable_diffusion/safety_checker.py')\n",
        "\n",
        "    def install_diffusers():\n",
        "      subprocess, os = Manager.manage_imports('diffuser_install')\n",
        "      settings = Colab.UserSettings.set_settings()\n",
        "      \n",
        "      print('Installing Transformers')\n",
        "      print(subprocess.run(['pip','install','transformers'], stdout=subprocess.PIPE).stdout.decode('utf-8'))\n",
        "      Colab.clear()\n",
        "      print('Installing Diffusers')\n",
        "      if settings['diffusers_version'] == 'latest':\n",
        "        print(subprocess.run(['pip','install','-U',f'git+https://github.com/huggingface/diffusers.git'], stdout=subprocess.PIPE).stdout.decode('utf-8'))\n",
        "      else:\n",
        "        print(subprocess.run(['pip','install','-U',f'git+https://github.com/huggingface/diffusers.git@{settings[\"diffusers_version\"]}'], stdout=subprocess.PIPE).stdout.decode('utf-8'))\n",
        "      Colab.clear()\n",
        "      print('Creating Directories')\n",
        "      print(subprocess.run(['mkdir','diffusers_output'], stdout=subprocess.PIPE).stdout.decode('utf-8'))\n",
        "      Colab.clear()\n",
        "      print('Installing Dependencies')\n",
        "      print(subprocess.run(['pip','install','pytorch-pretrained-bert','spacy','ftfy','scipy'], stdout=subprocess.PIPE).stdout.decode('utf-8'))\n",
        "      Colab.clear()\n",
        "      print('Populating Spacy')\n",
        "      print(subprocess.run(['python','-m','space','download','en'], stdout=subprocess.PIPE).stdout.decode('utf-8'))\n",
        "      Colab.clear()\n",
        "      print('Logging into Huggingface')\n",
        "      username, token = Manager.Diffusion.creds()\n",
        "      subprocess.run(['git', 'config', '--global', 'credential.helper', 'store'], stdout=subprocess.DEVNULL)\n",
        "      left_of_pipe = subprocess.Popen([\"echo\", token], stdout=subprocess.PIPE)\n",
        "      right_of_pipe = subprocess.run(['huggingface-cli', 'login'], stdin=left_of_pipe.stdout, stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
        "      # Manager.Diffusion.install_model(username, token, settings[\"model_id\"])\n",
        "      Colab.clear()\n",
        "      if not os.path.exists('/content/safety_checker_patched.py'):\n",
        "        print('Creating Patches')\n",
        "        print(subprocess.run(['cp','/usr/local/lib/python3.7/dist-packages/diffusers/pipelines/stable_diffusion/safety_checker.py','/content/safety_checker.py'], stdout=subprocess.PIPE).stdout.decode('utf-8'))\n",
        "        print(subprocess.run(['cp','/content/safety_checker.py','/content/safety_checker_patched.py'], stdout=subprocess.PIPE).stdout.decode('utf-8'))\n",
        "        with open(f'/content/safety_checker_patched.py','r') as unpatched_file:\n",
        "          patch = unpatched_file.read().replace('for idx, has_nsfw_concept in enumerate(has_nsfw_concepts):','#for idx, has_nsfw_concept in enumerate(has_nsfw_concepts):').replace('if has_nsfw_concept:','# if has_nsfw_concept:').replace('images[idx] = np.zeros(images[idx].shape)  # black image', '# images[idx] = np.zeros(images[idx].shape)  # black image').replace(\"Potential NSFW content was detected in one or more images. A black image will be returned instead.\",\"Potential NSFW content was detected in one or more images. It's patched out, no actions were taken.\").replace(\" Try again with a different prompt and/or seed.\",\"\")\n",
        "        with open(f'/content/safety_checker_patched.py','w') as file:\n",
        "          file.write(patch)\n",
        "      with open('/content/CLIP_GUIDED.py', 'w') as file:\n",
        "        file.write('''\n",
        "import inspect\n",
        "from typing import List, Optional, Union\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "from diffusers import AutoencoderKL, DiffusionPipeline, LMSDiscreteScheduler, PNDMScheduler, UNet2DConditionModel\n",
        "from diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion import StableDiffusionPipelineOutput\n",
        "from torchvision import transforms\n",
        "from transformers import CLIPFeatureExtractor, CLIPModel, CLIPTextModel, CLIPTokenizer\n",
        "\n",
        "\n",
        "class MakeCutouts(nn.Module):\n",
        "    def __init__(self, cut_size, cut_power=1.0):\n",
        "        super().__init__()\n",
        "\n",
        "        self.cut_size = cut_size\n",
        "        self.cut_power = cut_power\n",
        "\n",
        "    def forward(self, pixel_values, num_cutouts):\n",
        "        sideY, sideX = pixel_values.shape[2:4]\n",
        "        max_size = min(sideX, sideY)\n",
        "        min_size = min(sideX, sideY, self.cut_size)\n",
        "        cutouts = []\n",
        "        for _ in range(num_cutouts):\n",
        "            size = int(torch.rand([]) ** self.cut_power * (max_size - min_size) + min_size)\n",
        "            offsetx = torch.randint(0, sideX - size + 1, ())\n",
        "            offsety = torch.randint(0, sideY - size + 1, ())\n",
        "            cutout = pixel_values[:, :, offsety : offsety + size, offsetx : offsetx + size]\n",
        "            cutouts.append(F.adaptive_avg_pool2d(cutout, self.cut_size))\n",
        "        return torch.cat(cutouts)\n",
        "\n",
        "\n",
        "def spherical_dist_loss(x, y):\n",
        "    x = F.normalize(x, dim=-1)\n",
        "    y = F.normalize(y, dim=-1)\n",
        "    return (x - y).norm(dim=-1).div(2).arcsin().pow(2).mul(2)\n",
        "\n",
        "\n",
        "def set_requires_grad(model, value):\n",
        "    for param in model.parameters():\n",
        "        param.requires_grad = value\n",
        "\n",
        "\n",
        "class CLIPGuidedStableDiffusion(DiffusionPipeline):\n",
        "    \"\"\"CLIP guided stable diffusion based on the amazing repo by @crowsonkb and @Jack000\n",
        "    - https://github.com/Jack000/glid-3-xl\n",
        "    - https://github.dev/crowsonkb/k-diffusion\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        vae: AutoencoderKL,\n",
        "        text_encoder: CLIPTextModel,\n",
        "        clip_model: CLIPModel,\n",
        "        tokenizer: CLIPTokenizer,\n",
        "        unet: UNet2DConditionModel,\n",
        "        scheduler: Union[PNDMScheduler, LMSDiscreteScheduler],\n",
        "        feature_extractor: CLIPFeatureExtractor,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        scheduler = scheduler.set_format(\"pt\")\n",
        "        self.register_modules(\n",
        "            vae=vae,\n",
        "            text_encoder=text_encoder,\n",
        "            clip_model=clip_model,\n",
        "            tokenizer=tokenizer,\n",
        "            unet=unet,\n",
        "            scheduler=scheduler,\n",
        "            feature_extractor=feature_extractor,\n",
        "        )\n",
        "\n",
        "        self.normalize = transforms.Normalize(mean=feature_extractor.image_mean, std=feature_extractor.image_std)\n",
        "        self.make_cutouts = MakeCutouts(feature_extractor.size)\n",
        "\n",
        "        set_requires_grad(self.text_encoder, False)\n",
        "        set_requires_grad(self.clip_model, False)\n",
        "\n",
        "    def enable_attention_slicing(self, slice_size: Optional[Union[str, int]] = \"auto\"):\n",
        "        if slice_size == \"auto\":\n",
        "            # half the attention head size is usually a good trade-off between\n",
        "            # speed and memory\n",
        "            slice_size = self.unet.config.attention_head_dim // 2\n",
        "        self.unet.set_attention_slice(slice_size)\n",
        "\n",
        "    def disable_attention_slicing(self):\n",
        "        self.enable_attention_slicing(None)\n",
        "\n",
        "    def freeze_vae(self):\n",
        "        set_requires_grad(self.vae, False)\n",
        "\n",
        "    def unfreeze_vae(self):\n",
        "        set_requires_grad(self.vae, True)\n",
        "\n",
        "    def freeze_unet(self):\n",
        "        set_requires_grad(self.unet, False)\n",
        "\n",
        "    def unfreeze_unet(self):\n",
        "        set_requires_grad(self.unet, True)\n",
        "\n",
        "    @torch.enable_grad()\n",
        "    def cond_fn(\n",
        "        self,\n",
        "        latents,\n",
        "        timestep,\n",
        "        index,\n",
        "        text_embeddings,\n",
        "        noise_pred_original,\n",
        "        text_embeddings_clip,\n",
        "        clip_guidance_scale,\n",
        "        num_cutouts,\n",
        "        use_cutouts=True,\n",
        "    ):\n",
        "        latents = latents.detach().requires_grad_()\n",
        "\n",
        "        if isinstance(self.scheduler, LMSDiscreteScheduler):\n",
        "            sigma = self.scheduler.sigmas[index]\n",
        "            # the model input needs to be scaled to match the continuous ODE formulation in K-LMS\n",
        "            latent_model_input = latents / ((sigma**2 + 1) ** 0.5)\n",
        "        else:\n",
        "            latent_model_input = latents\n",
        "\n",
        "        # predict the noise residual\n",
        "        noise_pred = self.unet(latent_model_input, timestep, encoder_hidden_states=text_embeddings).sample\n",
        "\n",
        "        if isinstance(self.scheduler, PNDMScheduler):\n",
        "            alpha_prod_t = self.scheduler.alphas_cumprod[timestep]\n",
        "            beta_prod_t = 1 - alpha_prod_t\n",
        "            # compute predicted original sample from predicted noise also called\n",
        "            # \"predicted x_0\" of formula (12) from https://arxiv.org/pdf/2010.02502.pdf\n",
        "            pred_original_sample = (latents - beta_prod_t ** (0.5) * noise_pred) / alpha_prod_t ** (0.5)\n",
        "\n",
        "            fac = torch.sqrt(beta_prod_t)\n",
        "            sample = pred_original_sample\n",
        "        elif isinstance(self.scheduler, LMSDiscreteScheduler):\n",
        "            sigma = self.scheduler.sigmas[index]\n",
        "            sample = latents - sigma * noise_pred\n",
        "        else:\n",
        "            raise ValueError(f\"scheduler type {type(self.scheduler)} not supported\")\n",
        "\n",
        "        sample = 1 / 0.18215 * sample\n",
        "        image = self.vae.decode(sample).sample\n",
        "        image = (image / 2 + 0.5).clamp(0, 1)\n",
        "\n",
        "        if use_cutouts:\n",
        "            image = self.make_cutouts(image, num_cutouts)\n",
        "        else:\n",
        "            image = transforms.Resize(self.feature_extractor.size)(image)\n",
        "        image = self.normalize(image)\n",
        "\n",
        "        image_embeddings_clip = self.clip_model.get_image_features(image).float()\n",
        "        image_embeddings_clip = image_embeddings_clip / image_embeddings_clip.norm(p=2, dim=-1, keepdim=True)\n",
        "\n",
        "        if use_cutouts:\n",
        "            dists = spherical_dist_loss(image_embeddings_clip, text_embeddings_clip)\n",
        "            dists = dists.view([num_cutouts, sample.shape[0], -1])\n",
        "            loss = dists.sum(2).mean(0).sum() * clip_guidance_scale\n",
        "        else:\n",
        "            loss = spherical_dist_loss(image_embeddings_clip, text_embeddings_clip).mean() * clip_guidance_scale\n",
        "\n",
        "        grads = -torch.autograd.grad(loss, latents)[0]\n",
        "\n",
        "        if isinstance(self.scheduler, LMSDiscreteScheduler):\n",
        "            latents = latents.detach() + grads * (sigma**2)\n",
        "            noise_pred = noise_pred_original\n",
        "        else:\n",
        "            noise_pred = noise_pred_original - torch.sqrt(beta_prod_t) * grads\n",
        "        return noise_pred, latents\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def __call__(\n",
        "        self,\n",
        "        prompt: Union[str, List[str]],\n",
        "        height: Optional[int] = 512,\n",
        "        width: Optional[int] = 512,\n",
        "        num_inference_steps: Optional[int] = 50,\n",
        "        guidance_scale: Optional[float] = 7.5,\n",
        "        clip_guidance_scale: Optional[float] = 100,\n",
        "        clip_prompt: Optional[Union[str, List[str]]] = None,\n",
        "        num_cutouts: Optional[int] = 4,\n",
        "        use_cutouts: Optional[bool] = True,\n",
        "        generator: Optional[torch.Generator] = None,\n",
        "        latents: Optional[torch.FloatTensor] = None,\n",
        "        output_type: Optional[str] = \"pil\",\n",
        "        return_dict: bool = True,\n",
        "    ):\n",
        "        if isinstance(prompt, str):\n",
        "            batch_size = 1\n",
        "        elif isinstance(prompt, list):\n",
        "            batch_size = len(prompt)\n",
        "        else:\n",
        "            raise ValueError(f\"`prompt` has to be of type `str` or `list` but is {type(prompt)}\")\n",
        "\n",
        "        if height % 8 != 0 or width % 8 != 0:\n",
        "            raise ValueError(f\"`height` and `width` have to be divisible by 8 but are {height} and {width}.\")\n",
        "\n",
        "        # get prompt text embeddings\n",
        "        text_input = self.tokenizer(\n",
        "            prompt,\n",
        "            padding=\"max_length\",\n",
        "            max_length=self.tokenizer.model_max_length,\n",
        "            truncation=True,\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "        text_embeddings = self.text_encoder(text_input.input_ids.to(self.device))[0]\n",
        "\n",
        "        if clip_guidance_scale > 0:\n",
        "            if clip_prompt is not None:\n",
        "                clip_text_input = self.tokenizer(\n",
        "                    clip_prompt,\n",
        "                    padding=\"max_length\",\n",
        "                    max_length=self.tokenizer.model_max_length,\n",
        "                    truncation=True,\n",
        "                    return_tensors=\"pt\",\n",
        "                ).input_ids.to(self.device)\n",
        "            else:\n",
        "                clip_text_input = text_input.input_ids.to(self.device)\n",
        "            text_embeddings_clip = self.clip_model.get_text_features(clip_text_input)\n",
        "            text_embeddings_clip = text_embeddings_clip / text_embeddings_clip.norm(p=2, dim=-1, keepdim=True)\n",
        "\n",
        "        # here `guidance_scale` is defined analog to the guidance weight `w` of equation (2)\n",
        "        # of the Imagen paper: https://arxiv.org/pdf/2205.11487.pdf . `guidance_scale = 1`\n",
        "        # corresponds to doing no classifier free guidance.\n",
        "        do_classifier_free_guidance = guidance_scale > 1.0\n",
        "        # get unconditional embeddings for classifier free guidance\n",
        "        if do_classifier_free_guidance:\n",
        "            max_length = text_input.input_ids.shape[-1]\n",
        "            uncond_input = self.tokenizer(\n",
        "                [\"\"] * batch_size, padding=\"max_length\", max_length=max_length, return_tensors=\"pt\"\n",
        "            )\n",
        "            uncond_embeddings = self.text_encoder(uncond_input.input_ids.to(self.device))[0]\n",
        "\n",
        "            # For classifier free guidance, we need to do two forward passes.\n",
        "            # Here we concatenate the unconditional and text embeddings into a single batch\n",
        "            # to avoid doing two forward passes\n",
        "            text_embeddings = torch.cat([uncond_embeddings, text_embeddings])\n",
        "\n",
        "        # get the initial random noise unless the user supplied it\n",
        "\n",
        "        # Unlike in other pipelines, latents need to be generated in the target device\n",
        "        # for 1-to-1 results reproducibility with the CompVis implementation.\n",
        "        # However this currently doesn't work in `mps`.\n",
        "        latents_device = \"cpu\" if self.device.type == \"mps\" else self.device\n",
        "        latents_shape = (batch_size, self.unet.in_channels, height // 8, width // 8)\n",
        "        if latents is None:\n",
        "            latents = torch.randn(\n",
        "                latents_shape,\n",
        "                generator=generator,\n",
        "                device=latents_device,\n",
        "            )\n",
        "        else:\n",
        "            if latents.shape != latents_shape:\n",
        "                raise ValueError(f\"Unexpected latents shape, got {latents.shape}, expected {latents_shape}\")\n",
        "        latents = latents.to(self.device)\n",
        "\n",
        "        # set timesteps\n",
        "        accepts_offset = \"offset\" in set(inspect.signature(self.scheduler.set_timesteps).parameters.keys())\n",
        "        extra_set_kwargs = {}\n",
        "        if accepts_offset:\n",
        "            extra_set_kwargs[\"offset\"] = 1\n",
        "\n",
        "        self.scheduler.set_timesteps(num_inference_steps, **extra_set_kwargs)\n",
        "\n",
        "        # if we use LMSDiscreteScheduler, let's make sure latents are multiplied by sigmas\n",
        "        if isinstance(self.scheduler, LMSDiscreteScheduler):\n",
        "            latents = latents * self.scheduler.sigmas[0]\n",
        "\n",
        "        for i, t in enumerate(self.progress_bar(self.scheduler.timesteps)):\n",
        "            # expand the latents if we are doing classifier free guidance\n",
        "            latent_model_input = torch.cat([latents] * 2) if do_classifier_free_guidance else latents\n",
        "            if isinstance(self.scheduler, LMSDiscreteScheduler):\n",
        "                sigma = self.scheduler.sigmas[i]\n",
        "                # the model input needs to be scaled to match the continuous ODE formulation in K-LMS\n",
        "                latent_model_input = latent_model_input / ((sigma**2 + 1) ** 0.5)\n",
        "\n",
        "            # # predict the noise residual\n",
        "            noise_pred = self.unet(latent_model_input, t, encoder_hidden_states=text_embeddings).sample\n",
        "\n",
        "            # perform classifier free guidance\n",
        "            if do_classifier_free_guidance:\n",
        "                noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
        "                noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
        "\n",
        "            # perform clip guidance\n",
        "            if clip_guidance_scale > 0:\n",
        "                text_embeddings_for_guidance = (\n",
        "                    text_embeddings.chunk(2)[0] if do_classifier_free_guidance else text_embeddings\n",
        "                )\n",
        "                noise_pred, latents = self.cond_fn(\n",
        "                    latents,\n",
        "                    t,\n",
        "                    i,\n",
        "                    text_embeddings_for_guidance,\n",
        "                    noise_pred,\n",
        "                    text_embeddings_clip,\n",
        "                    clip_guidance_scale,\n",
        "                    num_cutouts,\n",
        "                    use_cutouts,\n",
        "                )\n",
        "\n",
        "            # compute the previous noisy sample x_t -> x_t-1\n",
        "            if isinstance(self.scheduler, LMSDiscreteScheduler):\n",
        "                latents = self.scheduler.step(noise_pred, i, latents).prev_sample\n",
        "            else:\n",
        "                latents = self.scheduler.step(noise_pred, t, latents).prev_sample\n",
        "\n",
        "        # scale and decode the image latents with vae\n",
        "        latents = 1 / 0.18215 * latents\n",
        "        image = self.vae.decode(latents).sample\n",
        "\n",
        "        image = (image / 2 + 0.5).clamp(0, 1)\n",
        "        image = image.cpu().permute(0, 2, 3, 1).numpy()\n",
        "\n",
        "        if output_type == \"pil\":\n",
        "            image = self.numpy_to_pil(image)\n",
        "\n",
        "        if not return_dict:\n",
        "            return (image, None)\n",
        "\n",
        "        return StableDiffusionPipelineOutput(images=image, nsfw_content_detected=None)\n",
        "\n",
        "        ''')\n",
        "      Manager.Diffusion.patch_nsfw(settings['enable_nsfw_filter'])\n",
        "      Colab.clear()\n",
        "\n",
        "\n",
        "    def install_model(username, token, model_id):\n",
        "      subprocess, os = Manager.manage_imports('diffuser_install')\n",
        "      print('Installing Model')\n",
        "      print(subprocess.run(['git','lfs','install'], stdout=subprocess.PIPE).stdout.decode('utf-8'))\n",
        "      # This will take a while\n",
        "      print(subprocess.run(['git','lfs','clone',f'https://{username}:{token}@huggingface.co/{model_id}'], stdout=subprocess.PIPE).stdout.decode('utf-8'))\n",
        "\n",
        "    def creds():\n",
        "      return 'replace', 'replace'\n",
        "\n",
        "    def img2img_init(settings):\n",
        "      def preprocess(image):\n",
        "        import numpy as np\n",
        "        import torch\n",
        "        import PIL.Image\n",
        "        w, h = image.size\n",
        "        w, h = map(lambda x: x - x % 32, (w, h))  # resize to integer multiple of 32\n",
        "        image = image.resize((w, h), resample=PIL.Image.LANCZOS)\n",
        "        image = np.array(image).astype(np.float32) / 255.0\n",
        "        image = image[None].transpose(0, 3, 1, 2)\n",
        "        image = torch.from_numpy(image)\n",
        "        return 2.*image - 1.\n",
        "      global pipe\n",
        "      import PIL.Image\n",
        "      import requests\n",
        "      if 'http' in settings['init_image']:\n",
        "        from io import BytesIO\n",
        "        response = requests.get(settings['init_image'])\n",
        "        init_image = PIL.Image.open(BytesIO(response.content))\n",
        "      elif settings['init_image'] is None or settings['init_image'] == \"\":\n",
        "        init_image = Colab.Images.Painter.img2img(settings['width'], settings['height'])\n",
        "      else:\n",
        "        #it's a file\n",
        "        init_image = PIL.Image.open(settings['init_image'])\n",
        "      print(\"Init Image (automatically resized to match user input)\")\n",
        "      init_image = init_image.resize((settings['width'], settings['height']))\n",
        "      display(init_image)\n",
        "      init_image = preprocess(init_image.convert(\"RGB\"))\n",
        "      return init_image\n",
        "    \n",
        "    def inpaint_init(settings):\n",
        "      import PIL.Image\n",
        "      def download(location):\n",
        "        from io import BytesIO\n",
        "        import requests\n",
        "        import PIL.Image\n",
        "        response = requests.get(location)\n",
        "        return PIL.Image.open(BytesIO(response.content))\n",
        "      if 'http' in settings[\"inpaint_image\"]:\n",
        "        init_image = download(settings[\"inpaint_image\"])\n",
        "      else:\n",
        "        init_image = PIL.Image.open(settings[\"inpaint_image\"])\n",
        "      init_image = init_image.resize((settings['width'], settings['height']))\n",
        "      if 'http' in settings[\"mask_image\"]:\n",
        "        mask_image = download(settings[\"mask_image\"])\n",
        "      elif settings[\"mask_image\"]:\n",
        "        mask_image = PIL.Image.open(settings[\"mask_image\"])\n",
        "      else:\n",
        "        init_image.save(\"init.jpg\")\n",
        "        mask_image = Colab.Images.Painter.inpaint(settings['width'], settings['height'])\n",
        "      mask_image.resize((settings['width'], settings['height']))\n",
        "      return init_image, mask_image\n",
        "\n",
        "    class Scheduler:\n",
        "      def make(settings):\n",
        "        scheduler = Manager.manage_imports(settings[\"scheduler\"])\n",
        "        global pipe\n",
        "        if settings[\"scheduler\"] == 'default' or settings[\"scheduler\"] == 'pndm':\n",
        "          pipe.scheduler = scheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", num_train_timesteps=1000, skip_prk_steps=True)\n",
        "        elif settings[\"scheduler\"] == 'k-lms':\n",
        "          pipe.scheduler = scheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", num_train_timesteps=1000)\n",
        "        elif settings[\"scheduler\"] == 'ddim':\n",
        "          pipe.scheduler = scheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", clip_sample=False, set_alpha_to_one=False)\n",
        "\n",
        "    class Runner:\n",
        "\n",
        "      def run(settings):\n",
        "        def sharpen_mage(image, samples=1):\n",
        "          from PIL import ImageFilter\n",
        "          im = image\n",
        "          for i in range(samples):\n",
        "              im = im.filter(ImageFilter.SHARPEN)\n",
        "          return im\n",
        "        import time\n",
        "        torch, precision_scope, randint, sys = Manager.Diffusion.Runner.get_general_imports(settings)\n",
        "        with torch.no_grad():\n",
        "          with precision_scope(\"cuda\"):\n",
        "            if settings['seed'] == 0:\n",
        "              settings['seed'] = randint(0,sys.maxsize)\n",
        "            generator = torch.Generator(\"cuda\").manual_seed(settings['seed'])\n",
        "            counter = 1\n",
        "            clean_counter = 0\n",
        "            running = True\n",
        "            if settings[\"use_drive_for_pics\"]:\n",
        "              outdir = f'/content/drive/MyDrive/{settings[\"drive_pic_dir\"]}'\n",
        "            else:\n",
        "              outdir = '/content/diffusers_output'\n",
        "            epoch_time = int(time.time())\n",
        "            if settings[\"save_prompt_details\"]:\n",
        "              with open(f'{outdir}/{epoch_time}_prompt.json', 'w') as file:\n",
        "                import json\n",
        "                file.write(json.dumps(settings, indent=2))\n",
        "            if settings['mode'] == \"IMG2IMG\":\n",
        "              init_image = Manager.Diffusion.img2img_init(settings)\n",
        "            elif settings['mode'] == 'Inpainting':\n",
        "              init_image, mask_image = Manager.Diffusion.inpaint_init(settings)\n",
        "            while running:\n",
        "              Cleaner.clean_env()\n",
        "              if settings[\"mode\"] == \"PROMPT\":\n",
        "                if settings['prompt_type'] == 'TEXT':\n",
        "                  image = Manager.Diffusion.Runner.text_prompt(settings, torch, generator)\n",
        "                else:\n",
        "                  for prompt in settings[\"file_prompt\"]:\n",
        "                    pass\n",
        "                    # TODO\n",
        "              elif settings[\"mode\"] == \"IMG2IMG\":\n",
        "                image = Manager.Diffusion.Runner.img_to_img(settings, torch, generator, init_image)\n",
        "              elif settings[\"mode\"] == \"Inpainting\":\n",
        "                image = Manager.Diffusion.Runner.inpainting(settings, torch, generator, init_image, mask_image)\n",
        "              elif settings[\"mode\"] == \"CLIP GUIDED PROMPT\":\n",
        "                Cleaner.clean_env()\n",
        "                image = Manager.Diffusion.Runner.clip_guided_prompt(settings, torch, generator)\n",
        "                Cleaner.clean_env()\n",
        "              if settings['image_upscaler'] in ['None','IMG2IMG'] or not settings[\"delete_originals\"]:\n",
        "                image.save(f'{outdir}/{epoch_time}_seed_{settings[\"seed\"]}_original.png')\n",
        "                epoch_time = int(time.time())\n",
        "              clean_counter += 1\n",
        "              if settings[\"clean_iters\"] <= clean_counter:\n",
        "                Colab.clear()\n",
        "                clean_counter = 0\n",
        "              print(f'Image {counter}. SEED: {settings[\"seed\"]}')\n",
        "              display(image)\n",
        "              print('Enhancing and Upscaling')\n",
        "              if settings['image_upscaler'] != 'None':\n",
        "                image = Upscalers.check_upscalers(settings,image)\n",
        "                if settings['image_upscaler'] == 'IMG2IMG':\n",
        "                  image = image.resize((settings['width']*settings[\"upscale_amount\"], settings['height']*settings[\"upscale_amount\"]))\n",
        "                if settings['sharpen_amount'] > 0:\n",
        "                  image = sharpen_mage(image, settings['sharpen_amount'])\n",
        "                  if not settings[\"bulky_skip\"]:\n",
        "                    display(image)\n",
        "                  image.save(f'{outdir}/{epoch_time}_seed_{settings[\"seed\"]}_upscaled_{settings[\"upscale_amount\"]}_sharpened_{settings[\"sharpen_amount\"]}.png')\n",
        "                  epoch_time = int(time.time())\n",
        "                else:\n",
        "                  if not settings[\"bulky_skip\"]:\n",
        "                    display(image)\n",
        "                  image.save(f'{outdir}/{epoch_time}_seed_{settings[\"seed\"]}_upscaled_{settings[\"upscale_amount\"]}.png')\n",
        "                  epoch_time = int(time.time())\n",
        "              if settings[\"img2img_postprocess\"] or settings['image_upscaler'] == 'IMG2IMG':\n",
        "                if settings['image_upscaler'] == 'IMG2IMG':\n",
        "                  image = image.resize((settings['width']*settings[\"upscale_amount\"], settings['height']*settings[\"upscale_amount\"]))\n",
        "                image = Manager.Diffusion.Runner.img2img_postprocess(settings, image, generator)\n",
        "                if not settings[\"bulky_skip\"]:\n",
        "                  display(image)\n",
        "                image.save(f'{outdir}/{epoch_time}_seed_{settings[\"seed\"]}_upscaled_{settings[\"upscale_amount\"]}_img2imgenhanced.png')\n",
        "              if not settings['keep_seed']:\n",
        "                settings['seed'] += 1\n",
        "                generator = torch.Generator(\"cuda\").manual_seed(settings['seed'])\n",
        "\n",
        "              if not settings['run_forever']:\n",
        "                if counter >= settings['num_iters']:\n",
        "                  running = False\n",
        "                  \n",
        "              counter += 1\n",
        "\n",
        "      def img2img_postprocess(settings, image, generator):\n",
        "        import torch\n",
        "        print(\"running img2img postprocessing. Switching to img2img pipe\")\n",
        "        global pipe\n",
        "        pipe = None\n",
        "        Cleaner.clean_env()\n",
        "        pipe_library = Manager.manage_imports(\"IMG2IMG\")\n",
        "        import os, subprocess, torch\n",
        "        username, token = Manager.Diffusion.creds()\n",
        "        subprocess.run(['git', 'config', '--global', 'credential.helper', 'store'], stdout=subprocess.DEVNULL)\n",
        "        left_of_pipe = subprocess.Popen([\"echo\", token], stdout=subprocess.PIPE)\n",
        "        right_of_pipe = subprocess.run(['huggingface-cli', 'login'], stdin=left_of_pipe.stdout, stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
        "        if settings[\"low_vram_patch\"]:\n",
        "          try:\n",
        "            local_pipe = pipe_library.from_pretrained(settings['model_id'], revision=\"fp16\", torch_dtype=torch.float16, use_auth_token=True).to(\"cuda\")\n",
        "          except OSError:\n",
        "            local_pipe = pipe_library.from_pretrained(settings['model_id'], use_auth_token=True).to(\"cuda\")\n",
        "        else:\n",
        "          local_pipe = pipe_library.from_pretrained(settings['model_id'], use_auth_token=True).to(\"cuda\")\n",
        "        if settings[\"vram_over_speed\"]:\n",
        "          local_pipe.enable_attention_slicing()\n",
        "          Cache.Pipe.optimize_attention(local_pipe.unet)\n",
        "          \n",
        "        pipe = local_pipe\n",
        "        local_pipe = None\n",
        "        del local_pipe\n",
        "        Cleaner.clean_env()\n",
        "        scheduler = Manager.manage_imports(settings[\"scheduler\"])\n",
        "        if settings[\"vram_over_speed\"]:\n",
        "          pipe.enable_attention_slicing()\n",
        "          Cache.Pipe.optimize_attention(pipe.unet)\n",
        "        if settings[\"scheduler\"] == 'default' or settings[\"scheduler\"] == 'pndm':\n",
        "          pipe.scheduler = scheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", num_train_timesteps=1000, skip_prk_steps=True)\n",
        "        elif settings[\"scheduler\"] == 'k-lms':\n",
        "          pipe.scheduler = scheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", num_train_timesteps=1000)\n",
        "        elif settings[\"scheduler\"] == 'ddim':\n",
        "          pipe.scheduler = scheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", clip_sample=False, set_alpha_to_one=False)\n",
        "        img2img_settings = {\n",
        "            \"text_prompt\":settings['img2img'][\"text_prompt\"],\n",
        "            'init_strength':settings['img2img'][\"init_strength\"],\n",
        "            'scheduler':settings['scheduler'],\n",
        "            'scale':settings['scale'],\n",
        "            'steps':settings['img2img'][\"steps\"]\n",
        "        }\n",
        "        image = Manager.Diffusion.Runner.img_to_img(img2img_settings, torch, generator, image)\n",
        "        pipe = None\n",
        "        del pipe\n",
        "        print('Switching back to old pipe and then displaying the image')\n",
        "        Cleaner.clean_env()\n",
        "        Cache.Pipe.make(settings)\n",
        "        Manager.Diffusion.Scheduler.make(settings)\n",
        "        return image\n",
        "\n",
        "      def get_general_imports(settings):\n",
        "        torch, randint, sys = Manager.manage_imports('general_diffusion_run')\n",
        "        if settings['precision'] == 'autocast':\n",
        "          return torch, torch.autocast, randint, sys\n",
        "        else:\n",
        "          from contextlib import nullcontext\n",
        "          return torch, nullcontext, randint, sys\n",
        "\n",
        "      def text_prompt(settings, torch, generator):\n",
        "        global pipe\n",
        "        if settings['scheduler'] == 'ddim':\n",
        "          image = pipe(prompt=settings['text_prompt'], num_inference_steps=settings['steps'], width=settings['width'], height=settings['height'], guidance_scale=settings['scale'], eta=settings[\"ddim_eta\"], generator=generator)\n",
        "        else:\n",
        "          image = pipe(prompt=settings['text_prompt'], num_inference_steps=settings['steps'], width=settings['width'], height=settings['height'], guidance_scale=settings['scale'], generator=generator)\n",
        "        return image[\"sample\"][0]\n",
        "        \n",
        "\n",
        "      def file_prompt(settings, torch, generator):\n",
        "        global pipe\n",
        "        pass\n",
        "\n",
        "      def img_to_img(settings, torch, generator, init_image):\n",
        "        if settings['scheduler'] == 'ddim':\n",
        "          image = pipe(prompt=settings['text_prompt'], num_inference_steps=settings['steps'], init_image=init_image, strength=settings['init_strength'], eta=settings[\"ddim_eta\"], guidance_scale=settings['scale'], generator=generator)[\"sample\"][0]\n",
        "        else:\n",
        "          image = pipe(prompt=settings['text_prompt'], num_inference_steps=settings['steps'], init_image=init_image, strength=settings['init_strength'], guidance_scale=settings['scale'], generator=generator)[\"sample\"][0]\n",
        "        return image\n",
        "\n",
        "      def inpainting(settings, torch, generator, init_image, mask_image):\n",
        "        global pipe\n",
        "        if settings['scheduler'] == 'ddim':\n",
        "          image = pipe(prompt=settings['text_prompt'], num_inference_steps=settings['steps'], init_image=init_image, mask_image=mask_image, strength=settings[\"inpaint_strength\"], eta=settings[\"ddim_eta\"], guidance_scale=settings['scale'], generator=generator)[\"sample\"][0]\n",
        "        else:\n",
        "          image = pipe(prompt=settings['text_prompt'], num_inference_steps=settings['steps'], init_image=init_image, mask_image=mask_image, strength=settings[\"inpaint_strength\"], guidance_scale=settings['scale'], generator=generator)[\"sample\"][0]\n",
        "        return image\n",
        "\n",
        "      def clip_guided_prompt(settings, torch, generator):\n",
        "        global pipe\n",
        "        if settings[\"unfreeze_unet\"] == \"True\":\n",
        "          pipe.unfreeze_unet()\n",
        "        else:\n",
        "          pipe.freeze_unet()\n",
        "\n",
        "        if settings[\"unfreeze_vae\"] == \"True\":\n",
        "         pipe.unfreeze_vae()\n",
        "        else:\n",
        "          pipe.freeze_vae()\n",
        "        use_cutouts = False\n",
        "        if settings[\"clip_cutouts\"] >= 1:\n",
        "          use_cutouts = True\n",
        "        Cleaner.clean_env()\n",
        "        image = pipe(\n",
        "            settings[\"text_prompt\"],\n",
        "            clip_prompt=settings[\"clip_prompt\"] if settings[\"clip_prompt\"].strip() != \"\" else None,\n",
        "            num_inference_steps=settings[\"steps\"],\n",
        "            guidance_scale=settings[\"scale\"], \n",
        "            clip_guidance_scale=settings[\"clip_guidance_scale\"],\n",
        "            num_cutouts=settings[\"clip_cutouts\"],\n",
        "            use_cutouts=use_cutouts == \"True\",\n",
        "            generator=generator,\n",
        "            width=settings[\"width\"],\n",
        "            height=settings[\"height\"]\n",
        "        ).images[0]\n",
        "        return image\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  Cleaner.clean_env()\n",
        "  manager = Manager()\n",
        "  # makes pipe and schedulers\n",
        "  manager.eval_settings()\n",
        "  # make images\n",
        "  Manager.Diffusion.Runner.run(manager.colab.settings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "FxvIfgi9YH4a"
      },
      "outputs": [],
      "source": [
        "#@title <font color=\"orange\">**Clean Environment Up**</font>\n",
        "#@markdown <font size=\"3\">**Soft Reset** the environment by deleting pipes, models, and image handlers from memory. Use this when the VRAM gets stuck after changing pipes<br><br>\n",
        "#@markdown **Note:** Before using this cell, give a minute for the system itself to flush some stuff. This will give a higher chance of this function working.<br>\n",
        "#@markdown **Note 2:** Sometimes you'll get a persistent OOM bug when the GPU has been unallocated from your session. This is common with the new (09/2022) Free Colab Sessions</font>\n",
        "\n",
        "try:\n",
        "    del pipe; del transform; del prediction; del input_batch; del depth; del depth_image; del image; del sr_image; del enhanced_image; del img; del init; del original_init\n",
        "except NameError:\n",
        "    pass\n",
        "finally:\n",
        "    clean_env()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-cd15ZYjLqf"
      },
      "source": [
        "## Diffuser Experiments (run through the Diffuser setup first)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "jdSQSFlRjONj"
      },
      "outputs": [],
      "source": [
        "#@title Modifier Tester\n",
        "#@markdown `MODIFIER_FILE`: location of a text file with a list of modifiers seperated by newline.<br>You will need to upload it. Then right click on the file in colab and then click \"copy path\" (If you can't find it, click on the folder icon on the left pane). Then paste it in the box. See modifier_examples.txt for an example (if you're lazy, just edit the file. It will populate AFTER the first run)<br>\n",
        "#@markdown `BASE_PROMPT`: the prompt against which the modifiers will be tested<br>\n",
        "with open('modifier_examples.txt','w') as file:\n",
        "  file.write('Canon\\nNikon\\nPanasonic\\nSony\\nDigital Painting\\nMatte Painting\\nDrawing from a 5 year old\\nPasta Art\\nI made this while on acid')\n",
        "\n",
        "MODIFIER_FILE = \"/content/modifier_examples.txt\" #@param {type:'string'}\n",
        "BASE_PROMPT = \"A dog playing in a field\" #@param {type:'string'}\n",
        "STEPS = 50 #@param {type:\"slider\", min:5, max:500, step:5} \n",
        "SEED = 42 #@param {type:'integer'}\n",
        "WIDTH = 512 #@param {type:\"slider\", min:256, max:1920, step:64}\n",
        "HEIGHT = 512 #@param {type:\"slider\", min:256, max:1920, step:64}\n",
        "SCALE = 13.8 #@param {type:\"slider\", min:0, max:25, step:0.1}\n",
        "PRECISION = \"autocast\" #@param [\"full\",\"autocast\"]\n",
        "SEED = torch.Generator(\"cuda\").manual_seed(SEED)\n",
        "import random\n",
        "import torch\n",
        "from contextlib import contextmanager, nullcontext\n",
        "import time\n",
        "import os\n",
        "from torch import autocast\n",
        "\n",
        "OUTDIR = '/content/experiments'\n",
        "precision_scope = autocast if PRECISION==\"autocast\" else nullcontext\n",
        "with open(MODIFIER_FILE) as file:\n",
        "  for line in file.readlines():\n",
        "    line = line.strip()\n",
        "    PROMPT = f\"{BASE_PROMPT}, {line}\"\n",
        "    print(f\"Running: {PROMPT}\")\n",
        "    image = pipe(PROMPT, num_inference_steps=STEPS, width=int(WIDTH), height=int(HEIGHT), guidance_scale=SCALE, generator=SEED)[\"sample\"][0]  \n",
        "    display(image)\n",
        "    try:\n",
        "      image.save(f'{OUTDIR}/modifier_{line}_scale_{SCALE}_steps_{STEPS}_seed_{SEED}.png')\n",
        "    except FileNotFoundError:\n",
        "      !mkdir $OUTDIR\n",
        "      image.save(f'{OUTDIR}/modifier_{line}_scale_{SCALE}_steps_{STEPS}_seed_{SEED}.png')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Q9upICRkMQcx"
      },
      "outputs": [],
      "source": [
        "#@title Randomizer, aka: I feel lucky/Fuck my shit up\n",
        "import os\n",
        "import random\n",
        "WORDS_AMOUNT = 30 #@param {type:\"integer\"}\n",
        "STEPS = 90 #@param {type:\"slider\", min:5, max:500, step:5}\n",
        "WIDTH = 512 #@param {type:\"slider\", min:256, max:1920, step:64}\n",
        "HEIGHT = 512 #@param {type:\"slider\", min:256, max:1920, step:64}\n",
        "SCALE = 13.7 #@param {type:\"slider\", min:0, max:25, step:0.1}\n",
        "PRECISION = \"autocast\" #@param [\"full\",\"autocast\"]\n",
        "precision_scope = autocast if PRECISION==\"autocast\" else nullcontext\n",
        "\n",
        "if not os.path.exists('words.txt'):\n",
        "  !wget https://gist.githubusercontent.com/h3xx/1976236/raw/bbabb412261386673eff521dddbe1dc815373b1d/wiki-100k.txt -O words.txt\n",
        "with open('words.txt') as file:\n",
        "  words = file.readlines()\n",
        "  prompt = \"\"\n",
        "  for iteration in range(WORDS_AMOUNT):\n",
        "    again = True\n",
        "    while again:\n",
        "      word = random.choice(words).strip()\n",
        "      if not '#' in word:\n",
        "        again = False\n",
        "    prompt += f'{random.choice(words).strip()}, '\n",
        "prompt = prompt[:-2]\n",
        "print(f'Prompt: {prompt}')\n",
        "with precision_scope(\"cuda\"):\n",
        "  seed = torch.Generator(\"cuda\").manual_seed(random.randint(0,4294967295))\n",
        "  image = pipe(prompt, num_inference_steps=STEPS, width=int(WIDTH), height=int(HEIGHT), guidance_scale=SCALE, generator=gen_seed)[\"sample\"][0] \n",
        "  display(image)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WcnY9hUsy76f"
      },
      "source": [
        "#TXT2IMG Method (Needs fixing, diffusers right now is my priority)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "BKZThsccD4Da"
      },
      "outputs": [],
      "source": [
        "#@title Huggingface Login\n",
        "from getpass import getpass\n",
        "\n",
        "huggingface_username = '' #@param {type:\"string\"}\n",
        "huggingface_token = '' #@param {type:\"string\"}\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "-HMZ9IiRDs8Q"
      },
      "outputs": [],
      "source": [
        "#@title TXT2IMG Setup\n",
        "import os\n",
        "!pip install -q condacolab\n",
        "import condacolab\n",
        "import torch\n",
        "torch.cuda.empty_cache()\n",
        "condacolab.install()\n",
        "root_code = root_model = \"/content/stableai\"\n",
        "code_dir = root_code + \"/stable-diffusion\"\n",
        "import os\n",
        "if not os.path.isdir(root_code):\n",
        "  !mkdir $root_code\n",
        "%cd $root_code\n",
        "!git clone https://github.com/DamascusGit/stable-diffusion.git\n",
        "!mamba env update -n base -f stable-diffusion/environment.yaml\n",
        "!pip install torchmetrics==0.6.0\n",
        "!pip install kornia==0.6)\n",
        "if not os.path.isdir(root_model):\n",
        "  !mkdir $root_model\n",
        "%cd $root_model\n",
        "!git lfs install\n",
        "!GIT_LFS_SKIP_SMUDGE=0\n",
        "# Will take a long time\n",
        "!git lfs clone https://$huggingface_username:$huggingface_token@huggingface.co/CompVis/stable-diffusion-v-1-4-original\n",
        "import numpy as np\n",
        "import os\n",
        "from PIL import Image\n",
        "def display_last_grid(grid_dir):\n",
        "  dir_list = os.listdir(grid_dir)\n",
        "  dir_list.sort()\n",
        "  #print (dir_list)\n",
        "  last_image = dir_list[-2]\n",
        "  img = Image.open(grid_dir + \"/\" + last_image).convert('RGB')\n",
        "  target_size = 600\n",
        "  img.thumbnail((target_size,target_size))\n",
        "  display (img)\n",
        "!mkdir /content/txt2img_output\n",
        "%cd $code_dir"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9QnhfmAM0t-X"
      },
      "outputs": [],
      "source": [
        "\n",
        "import random\n",
        "import torch\n",
        "from contextlib import contextmanager, nullcontext\n",
        "import time\n",
        "import os\n",
        "from torch import autocast\n",
        "torch.cuda.empty_cache()\n",
        "PROMPT = \"matte potrait of a young cyberpunk woman as a Disney Princess, full-frame, complex picture, intricate, fine details, vogue, trending on artstation, artgerm, greg manchess, studio ghibli, Disney, Star Wars\" #@param {type:'string'}\n",
        "STEPS = 160 #@param {type:\"slider\", min:5, max:500, step:5} \n",
        "SEED = 0 #@param {type:'integer'}\n",
        "NUM_ITERS = 6 #@param {type:\"slider\", min:1, max:100, step:1} \n",
        "WIDTH = 512 #@param {type:\"slider\", min:256, max:1920, step:64}\n",
        "HEIGHT = 512 #@param {type:\"slider\", min:256, max:1920, step:64}\n",
        "SCALE = 14 #@param {type:\"slider\", min:0, max:25, step:0.1}\n",
        "PRECISION = \"autocast\" #@param [\"full\",\"autocast\"]\n",
        "SAVE_PROMPT_DETAILS = True #@param {type:\"boolean\"}\n",
        "USE_DRIVE_FOR_PICS = True #@param {type:\"boolean\"}\n",
        "DRIVE_PIC_DIR = \"AI_PICS\" #@param {type:\"string\"}\n",
        "IMAGE_UPSCALER = True #@param {type:\"boolean\"}\n",
        "UPSCALE_AMOUNT = \"2\" #@param [\"2\",\"4\", \"8\"]\n",
        "precision_scope = autocast if PRECISION==\"autocast\" else nullcontext\n",
        "ORIG_SEED = SEED\n",
        "\n",
        "%cd /content/\n",
        "\n",
        "if IMAGE_UPSCALER:\n",
        "  def upscale(image):\n",
        "    sr_image = model.predict(np.array(image))\n",
        "    return sr_image\n",
        "  if not os.path.exists('/content/Real-ESRGAN'):\n",
        "    !git clone https://github.com/sberbank-ai/Real-ESRGAN\n",
        "    !pip install -r Real-ESRGAN/requirements.txt\n",
        "    !gdown https://drive.google.com/uc?id=1pG2S3sYvSaO0V0B8QPOl1RapPHpUGOaV -O Real-ESRGAN/weights/RealESRGAN_x2.pth\n",
        "    !gdown https://drive.google.com/uc?id=1SGHdZAln4en65_NQeQY9UjchtkEF9f5F -O Real-ESRGAN/weights/RealESRGAN_x4.pth\n",
        "    !gdown https://drive.google.com/uc?id=1mT9ewx86PSrc43b-ax47l1E2UzR7Ln4j -O Real-ESRGAN/weights/RealESRGAN_x8.pth\n",
        "  %cd /content/Real-ESRGAN\n",
        "  from realesrgan import RealESRGAN\n",
        "  from PIL import Image\n",
        "  import numpy as np\n",
        "  import torch\n",
        "\n",
        "  device = torch.device('cuda')\n",
        "  model = RealESRGAN(device, scale = int(UPSCALE_AMOUNT))\n",
        "  model.load_weights(f'weights/RealESRGAN_x{UPSCALE_AMOUNT}.pth')\n",
        "  %cd /content/\n",
        "\n",
        "if USE_DRIVE_FOR_PICS and not os.path.exists('/content/drive'):\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive')\n",
        "if USE_DRIVE_FOR_PICS and not os.path.exists(f'/content/drive/MyDrive/{DRIVE_PIC_DIR}'):\n",
        "  !mkdir /content/drive/MyDrive/$DRIVE_PIC_DIR\n",
        "if USE_DRIVE_FOR_PICS:\n",
        "  OUTDIR = f'/content/drive/MyDrive/{DRIVE_PIC_DIR}'\n",
        "else:\n",
        "  OUTDIR = '/content/diffusers_output'\n",
        "epoch_time = int(time.time())\n",
        "if SAVE_PROMPT_DETAILS:\n",
        "  with open(f'{OUTDIR}/{epoch_time}_prompt.txt', 'w') as file:\n",
        "        file.write(f'{PROMPT}\\n\\nHeight: {HEIGHT}\\nWidth: {WIDTH}\\nSeed: {SEED}\\nScale: {SCALE}\\nPrecision: {PRECISION}\\n')\n",
        "with precision_scope(\"cuda\"):\n",
        "  for iteration in range(NUM_ITERS):\n",
        "    \n",
        "    if ORIG_SEED == 0:\n",
        "      rand_num = random.randint(0,4294967295)\n",
        "      gen_seed = torch.Generator(\"cuda\").manual_seed(rand_num)\n",
        "    else:\n",
        "      gen_seed = torch.Generator(\"cuda\").manual_seed(SEED)\n",
        "    epoch_time = int(time.time())\n",
        "    try:\n",
        "      print(f'Seed: {rand_num}, Scale: {SCALE}, Steps: {STEPS}')\n",
        "    except NameError:\n",
        "      print(f'Seed: {SEED}, Scale: {SCALE}, Steps: {STEPS}')\n",
        "    \n",
        "    image = pipe(PROMPT, num_inference_steps=STEPS, width=int(WIDTH), height=int(HEIGHT), guidance_scale=SCALE, generator=gen_seed)[\"sample\"][0]  \n",
        "    display(image)\n",
        "    try:\n",
        "      image.save(f'{OUTDIR}/{str(epoch_time)}_scale_{SCALE}_steps_{STEPS}_seed_{rand_num}.png')\n",
        "    except NameError:\n",
        "      image.save(f'{OUTDIR}/{str(epoch_time)}_scale_{SCALE}_steps_{STEPS}_seed_{SEED}.png')\n",
        "    print('Upscaling... ')\n",
        "    sr_image = upscale(image)\n",
        "    display(sr_image)\n",
        "    try:\n",
        "      sr_image.save(f'{OUTDIR}/{str(epoch_time)}_scale_{SCALE}_steps_{STEPS}_seed_{rand_num}_upscaled_{UPSCALE_AMOUNT}.png')\n",
        "    except NameError:\n",
        "      sr_image.save(f'{OUTDIR}/{str(epoch_time)}_scale_{SCALE}_steps_{STEPS}_seed_{SEED}_upscaled_{UPSCALE_AMOUNT}.png')\n",
        "#@markdown If you're using the `low VRAM patch` you <b>HAVE</b> to use `autocast`<br>\n",
        "#@markdown `Out of Memory error`: If the VRAM stacks right after execution, sometimes it helps waiting for a minute before running it again. Looking at ways to force it to clear the VRAM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HQeX4o971T_U"
      },
      "source": [
        "#How to install offline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fFdp-FFg1cao"
      },
      "source": [
        "https://rentry.org/SDInstallation"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "WcnY9hUsy76f"
      ],
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}